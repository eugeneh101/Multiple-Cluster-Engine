{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decorator_1(func):\n",
    "    def func_wrapper(x):\n",
    "        print('dec1')\n",
    "        func(x)\n",
    "        print('dec1')\n",
    "    return func_wrapper\n",
    "\n",
    "def decorator_2(func):\n",
    "    def func_wrapper(x):\n",
    "        print('dec2')\n",
    "        func(x)\n",
    "        print('dec2')\n",
    "    return func_wrapper\n",
    "\n",
    "@decorator_2\n",
    "@decorator_1\n",
    "def function_a(x):\n",
    "    print(x)\n",
    "\n",
    "function_a(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_f(name1):\n",
    "    print('hello {}'.format(name1))\n",
    "\n",
    "def mapper_f(kwargs):\n",
    "    return temp_f(**kwargs)\n",
    "\n",
    "mapper_f({'name1': 'lee'})\n",
    "(lambda kwargs: temp_f(**kwargs))({'name1': 'lee'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !touch {i}.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "class MultipleClusterEnginePrototype(object):\n",
    "    def __init__(self, cluster_job_name, n_cpus_list, output_dir, file_names, function_to_process, function_kwargs_dict): # always put it in as a dictionary\n",
    "        self.cluster_job_name = cluster_job_name\n",
    "        self.n_cpus_list = n_cpus_list\n",
    "        self.output_dir = output_dir if output_dir[-1] == '/' else output_dir + '/'\n",
    "        self.file_names = file_names\n",
    "        self.function_to_process = lambda kwargs: function_to_process(**kwargs)\n",
    "        self.function_kwargs_dict = function_kwargs_dict\n",
    "        \n",
    "        assert cluster_job_name, \"Needs cluster name\"\n",
    "        assert len(n_cpus_list) > 0, \"Needs the number of CPUs per cluster\"\n",
    "        assert os.path.isdir(self.output_dir), \"Output directory doesn't exist\"\n",
    "        assert len(file_names) > 0, \"Need input files\"\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # used by engine\n",
    "        self.client_dict = {}\n",
    "        self.load_balanced_view_dict = {}\n",
    "        self.async_results_dict = defaultdict(list) # collects all the async_results\n",
    "        self.file_to_cluster_order_dict = defaultdict(list) # remembers which files are sent to which clusters\n",
    "        self.cluster_indexes = None\n",
    "        logging.basicConfig(filename='logger.log', level=logging.INFO)\n",
    "        self.logger = logging.getLogger('Willie says') # Easter egg\n",
    "        \n",
    "    def start_cluster(self, n_cpus, cluster_id):\n",
    "        self.logger.info('\\tAttempting to start {}{} with {} CPUs'.format(self.cluster_job_name, cluster_id, n_cpus))\n",
    "        os.system(\"ipcluster start --n={} --profile={}{} --daemonize\".format(\n",
    "            n_cpus, self.cluster_job_name, cluster_id)) # should deprecate to use a safer bash call\n",
    "\n",
    "        attempt_ctr = 0 \n",
    "        while attempt_ctr < 3: # Attempt to connect to client 3 times\n",
    "            time.sleep(10) # hard coded\n",
    "            try:\n",
    "                client = ipp.Client(profile='{}{}'.format(self.cluster_job_name, cluster_id))\n",
    "            except ipp.error.TimeoutError:\n",
    "                attempt_ctr += 1\n",
    "            else:\n",
    "                self.logger.info('\\t\\tCPU processes ready for action: {}'.format(client[:].apply_async(os.getpid).get()))\n",
    "                return client\n",
    "            # if there is any other error other than TimeoutError, then the error will be raised\n",
    "    \n",
    "    def start_all_clusters(self):\n",
    "        self.logger.info('Attempting to start all clusters')\n",
    "        for cluster_id, n_cpus in enumerate(self.n_cpus_list):\n",
    "            self.client_dict[cluster_id] = self.start_cluster(n_cpus, cluster_id)\n",
    "            self.load_balanced_view_dict[cluster_id] = self.client_dict[cluster_id].load_balanced_view()            \n",
    "        self.logger.info('All clusters started')\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        \n",
    "    def kill_cluster(self, cluster_id): # use better arguments\n",
    "        # client = client_list[cluster_id]\n",
    "        self.logger.info('\\tAttempting to kill {}{} with CPU processes: {}'.format(\n",
    "            self.cluster_job_name, cluster_id, self.client_dict[cluster_id][:].apply_async(os.getpid).get()))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        # client.purge_everything()\n",
    "        self.client_dict[cluster_id].close()\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.logger.info('\\t\\tCluster successfully killed')\n",
    "        time.sleep(5) # hard-coded\n",
    "        # have to mutate cluster_indexes\n",
    "        \n",
    "    def kill_all_clusters(self):\n",
    "        self.logger.info('Attempting to kill all clusters')\n",
    "        for cluster_id in self.client_dict:\n",
    "            self.kill_cluster(cluster_id)\n",
    "        self.logger.info('All clusters have been killed')\n",
    "        \n",
    "    def run_clusters(self):        \n",
    "        small_file_ctr = 1 # determine if you want to have queue or differently ordered queue\n",
    "        big_file_ctr = 0\n",
    "        \n",
    "\n",
    "        for ith_file in tqdm(range(len(self.file_names))):\n",
    "            for jth_cluster in self.cluster_indexes: # infinite loop\n",
    "                time.sleep(1) # hard coded delay time; want to do expected log time lag / number of clusters\n",
    "                ### insert code here to kill cluster if RAM usage too great, if possible log which file it was processing;\n",
    "                ### it has to do a global search of all clusters' RAM usage\n",
    "                ### would need a dictionary here to remember which cluster has which file; write to disk\n",
    "                ### profiler would also write to disk CPU usage what level\n",
    "\n",
    "                if (not self.async_results_dict[jth_cluster][-1:] \n",
    "                    or self.async_results_dict[jth_cluster][-1].done()): # check if cluster i is available                       \n",
    "                    # if necessary, recreate engine here\n",
    "                    if jth_cluster == 0: # Send large files to large cluster (ALWAYS has id == 0)\n",
    "                        index = big_file_ctr\n",
    "                        big_file_ctr += 1\n",
    "                    else: # Send small files to small clusters (ALWAYS have id > 0)\n",
    "                        index = -small_file_ctr\n",
    "                        small_file_ctr += 1\n",
    "                                              \n",
    "                                     \n",
    "                    # clear cluster memory \n",
    "\n",
    "                    # package_arguments\n",
    "                    function_kwargs_dict = copy.deepcopy(self.function_kwargs_dict)\n",
    "                    function_kwargs_dict.update({'file_name': self.file_names[index]}) \n",
    "                    \n",
    "                    ### insert code to write results to file--it will only have start times, no end times\n",
    "                    async_result = self.load_balanced_view_dict[jth_cluster].map_async(\n",
    "                            self.function_to_process, # function name\n",
    "#                            [self.file_names[index]] * len(self.client_dict[jth_cluster].ids), # file name, assumes first argument is always file name\n",
    "                            # [len(client_list[i].ids)] * len(client_list[i].ids), # number of CPUs, assumes second argument is always number of CPUs\n",
    "                            #  client_list[i].ids # CPU ids, assumes third argument is always CPU id; actually turn into kwargs\n",
    "                             # [output_folder_name] * len(client_list[i].ids) # assumes fourth argument is output directory\n",
    "#                            [self.function_kwargs_dict] * len(self.client_dict[jth_cluster].ids)                    \n",
    "                            [function_kwargs_dict] * len(self.client_dict[jth_cluster].ids)\n",
    "                            )                                              \n",
    "                    self.async_results_dict[jth_cluster].append(async_result)\n",
    "                    self.file_to_cluster_order_dict[jth_cluster].append(self.file_names[index])\n",
    "                    self.logger.info(\"{} is the {}th file and is sent to {}{} for processing\".format(self.file_names[index], ith_file, self.cluster_job_name, jth_cluster))\n",
    "                    break # break out of inner loop to determine if other clusters are available\n",
    "        # async_results_dict; save to disk for later inspection?\n",
    "        \n",
    "    def main(self):\n",
    "        start_time = datetime.now()\n",
    "        self.logger.info('Starting Multiple Cluster Engine at {}'.format(start_time))\n",
    "        self.start_all_clusters()\n",
    "        self.run_clusters()\n",
    "        self.kill_all_clusters()\n",
    "        end_time = datetime.now()\n",
    "        self.logger.info('Multiple Cluster Engine shut down at {}'.format(end_time))\n",
    "        self.logger.info('Total run time is {} minutes'.format((end_time - start_time).seconds / 60))\n",
    "        self.logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Logger' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d0e899775819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# mce.start_all_clusters()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# mce.run_clusters()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-5a1bad674230>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Multiple Cluster Engine shut down at {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total run time is {} minutes'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Logger' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "def fun_func(file_name, save_string_to_file):\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(save_string_to_file)\n",
    "\n",
    "mce_args = {\n",
    "    'cluster_job_name': 'write_to_file', # no spaces\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'function_to_process': fun_func,\n",
    "    'function_kwargs_dict': {'save_string_to_file': 'pee-a-boo!'},\n",
    "    'output_dir': '/home/ubuntu/cluster_results/' # use absolute path since it's safer\n",
    "    }\n",
    "\n",
    "mce = MultipleClusterEnginePrototype(**mce_args)\n",
    "# mce.start_all_clusters()\n",
    "# mce.run_clusters()\n",
    "mce.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='logger.log', level=logging.INFO)\n",
    "logger = logging.getLogger('Willie says') # Easter egg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logging.Logger"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mce.kill_all_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pee-a-boo!pee-a-boo!pee-a-boo!"
     ]
    }
   ],
   "source": [
    "!cat 9.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mce.async_results_dict[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to kill all clusters\n",
      "\tAttempting to kill mycluster0 with CPU processes: [2065, 2066, 2068, 2072]\n",
      "\t\tCluster successfully killed\n",
      "\tAttempting to kill mycluster1 with CPU processes: [2136, 2137, 2139]\n",
      "\t\tCluster successfully killed\n",
      "\tAttempting to kill mycluster2 with CPU processes: [2196, 2198]\n",
      "\t\tCluster successfully killed\n",
      "All clusters have been killed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mce.kill_all_clusters()\n",
    "#!ipcluster stop --profile=mycluster2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultipleClusterEngine(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.functions_to_run = functions_to_run\n",
    "        self.file_names = file_names\n",
    "        self.RAM_limit_in_GB = pass\n",
    "        # how to deal with other args/objects that functions might need\n",
    "        self.mce_job_name = None\n",
    "        pass\n",
    "   \n",
    "\n",
    "    def memory_profiler():\n",
    "        pass # if all clusters are dead, then raise Error with a message\n",
    "    \n",
    "    \n",
    "    def early_kill():\n",
    "        pass # write file to failure disk, maybe also cluster i and num_cpus\n",
    "    \n",
    "    def cluster_release_memory():\n",
    "        # after each map/reducer step, use gc.collect()\n",
    "        pass\n",
    "    \n",
    "    to_write = 'Processed ' + str(num_files) + ' files in ' \\\n",
    "            + str((datetime.today() - now).total_seconds() / 60) + ' minutes\\n'\n",
    "    to_write += 'Used ' + str(cpu_list[0]) + ' cpus to process ' + str(big_file_ctr) + \\\n",
    "                    ' big files and ' \n",
    "    # If there are multiple clusters, specify number of small files processed by small cluster\n",
    "    if len(cpu_list) > 1: \n",
    "        to_write += str(cpu_list[1]) + ' cpus to process ' + \\\n",
    "                    str(small_file_ctr) + ' small files.\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cluster is killed, then cannot trust jth_cluster index--instead of load_balanced_list, use load_balanced_dict\n",
    "\n",
    "# RAM writer and also progress/log writer and failre writer; use logging library\n",
    "# figure out queue vs deque; deque is better\n",
    "# # write a crap load of documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably no async or threading required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weakref\n",
    "# unittest with a mapper/reducer? after each map/reducer step, use gc.collect()\n",
    "\n",
    "# MCE works on files. Hence, if you don't have any datafiles, then just create some empty files\n",
    "# SSD for parallel reading (not HDD); determine if you are IO constrained\n",
    "# RAM usage is heavier in Python 3 than Python 2; though Python 3 memory management is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all engines killed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if function fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/donnemartin/data-science-ipython-notebooks/tree/master/mapreduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail -n 50 -f temp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
