{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting multiple_cluster_engine.py\n"
     ]
    }
   ],
   "source": [
    "%%file multiple_cluster_engine.py\n",
    "import ipyparallel as ipp\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import logging # logging can create duplicate entries if you don't reload logging\n",
    "try:\n",
    "    from importlib import reload # Python 3\n",
    "except: # Python 2 reload is a builtin\n",
    "    pass\n",
    "\n",
    "class MultipleClusterEngine(object):\n",
    "    def __init__(self, \n",
    "                 cluster_job_name, \n",
    "                 n_cpus_list, \n",
    "                 ram_limit_in_GB,\n",
    "                 wait_time_in_seconds,\n",
    "                 input_file_names,\n",
    "                 output_parent_dir,\n",
    "                 function_to_process,\n",
    "                 function_kwargs_dict): # always put function args in a dictionary\n",
    "        reload(logging)\n",
    "        self.cluster_job_name = cluster_job_name\n",
    "        self.n_cpus_list = n_cpus_list\n",
    "        self.ram_limit_in_GB = ram_limit_in_GB\n",
    "        self.wait_time_in_seconds = wait_time_in_seconds\n",
    "        self.output_parent_dir = output_parent_dir\n",
    "        self.input_file_names = input_file_names\n",
    "        self.function_to_process = lambda kwargs: function_to_process(**kwargs)\n",
    "        self.function_kwargs_dict = function_kwargs_dict\n",
    "        \n",
    "        assert cluster_job_name, \"Needs cluster name\"\n",
    "        assert len(n_cpus_list) > 0, \"Needs the number of CPUs per cluster\"\n",
    "        assert os.path.isdir(self.output_parent_dir), \"Output directory doesn't exist\"\n",
    "        assert len(self.input_file_names) > 0, \"Need input files\"\n",
    "\n",
    "        # used by engine\n",
    "        self.cluster_dict = {}\n",
    "        self.load_balanced_view_dict = {}\n",
    "        self.async_results_dict = defaultdict(list) # collects all the async_results\n",
    "        self.file_to_cluster_order_dict = defaultdict(list) # remembers which file is sent to which cluster\n",
    "        self.cluster_indexes = None\n",
    "        self.logger_status = None\n",
    "        self.logger_failure = None\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.cluster_output_dir = None\n",
    "        self.cluster_RAM_use_dict = {}\n",
    "        self.cluster_pid_dict = {}\n",
    "        self.n_unsuccessful_files = 0\n",
    "\n",
    "    def create_cluster_output_dir(self):\n",
    "        \"\"\"Creates the folder which all the results will be saved to based on\n",
    "        the output_parent_dir and the cluster job name and an incremented number\"\"\"\n",
    "        subdirs = [name for name in os.listdir(self.output_parent_dir) if \n",
    "                   os.path.isdir(os.path.join(self.output_parent_dir, name))]\n",
    "        existing_results_dir = []\n",
    "        for subdir in subdirs:\n",
    "            try:\n",
    "                existing_results_dir.append(int(subdir.strip(self.cluster_job_name)))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        dir_index = max(existing_results_dir) + 1 if existing_results_dir else 0\n",
    "        self.cluster_output_dir = os.path.join(self.output_parent_dir, \n",
    "                                               self.cluster_job_name + str(dir_index))\n",
    "        os.makedirs(self.cluster_output_dir)\n",
    "                \n",
    "    def create_logger(self, logger_name, log_file):\n",
    "        \"\"\"Create logger objects\"\"\"\n",
    "        l = logging.getLogger(logger_name)\n",
    "        fileHandler = logging.FileHandler(log_file)\n",
    "        l.addHandler(fileHandler)\n",
    "        l.setLevel(logging.INFO)\n",
    "    \n",
    "    def activate_logger(self):\n",
    "        \"\"\"Create a logger for status, failure, and RAM usage updates\"\"\"\n",
    "        self.create_logger('status', os.path.join(self.cluster_output_dir, \"status.log\"))\n",
    "        self.create_logger('failure', os.path.join(self.cluster_output_dir, \"failure.log\"))\n",
    "        self.create_logger('ram_usage', os.path.join(self.cluster_output_dir, \"ram_usage.log\"))\n",
    "        self.logger_status = logging.getLogger('status')\n",
    "        self.logger_status.propagate = False\n",
    "        self.logger_failure = logging.getLogger('failure')\n",
    "        self.logger_failure.propagate = False\n",
    "        self.logger_ram_usage = logging.getLogger('ram_usage')\n",
    "        self.logger_ram_usage.propagate = False\n",
    "        \n",
    "    def profile_memory_for_cluster(self, cluster_id): # RAM in GB\n",
    "        \"\"\"Given a cluster ID, determines how much RAM that cluster is using,\n",
    "        which is just the sum of the RAM attached the CPUs in the cluster\"\"\"\n",
    "        return sum(psutil.Process(pid).memory_info().rss for \n",
    "                   pid in self.cluster_pid_dict[cluster_id]) / 1e9\n",
    "        \n",
    "    def profile_memory_for_all_clusters(self):\n",
    "        \"\"\"Determines the RAM for all the clusters\"\"\"\n",
    "        self.cluster_RAM_use_dict.clear()\n",
    "        for jth_cluster in sorted(self.load_balanced_view_dict):\n",
    "            self.cluster_RAM_use_dict[jth_cluster] = self.profile_memory_for_cluster(jth_cluster)           \n",
    "            self.logger_ram_usage.info('{}: {}th cluster uses {} GB of RAM'.format(\n",
    "                datetime.now(), jth_cluster, self.cluster_RAM_use_dict[jth_cluster]))\n",
    "        self.logger_ram_usage.info('{}: All clusters use {} GB of RAM'.format(\n",
    "                datetime.now(), sum(self.cluster_RAM_use_dict.values())))\n",
    "        \n",
    "    def clear_memory_on_cluster(self, cluster_id): # not as effective as imagined\n",
    "        \"\"\"Ideally clears out the RAM when a cluster successfully processes a file.\n",
    "        In practice, it is hard to say how effective this is in clearing the RAM.\n",
    "        Fortunately, running this method is very fast\"\"\"\n",
    "        import gc\n",
    "        self.cluster_dict[cluster_id][:].apply_async(gc.collect)\n",
    "        \n",
    "    def start_cluster(self, n_cpus, cluster_id):\n",
    "        \"\"\"Given a cluster ID, start the cluster with that ID and then attach\n",
    "        to the cluster and then store the CPU PIDs cluster_pid_dict and then\n",
    "        log it in the status.log file\n",
    "        \"\"\"\n",
    "        self.logger_status.info(\"{}: \\tAttempting to start cluster job \"\n",
    "            \"{}'s {}th cluster with {} CPUs\".format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, n_cpus))\n",
    "        os.system(\"ipcluster start --n={} --profile={}{} --daemonize\".format(\n",
    "            n_cpus, self.cluster_job_name, cluster_id)) # should deprecate to use a safer OS call\n",
    "\n",
    "        attempt_ctr = 0 \n",
    "        while attempt_ctr < 3: # Attempt to connect to client 3 times\n",
    "            time.sleep(10) # hard coded\n",
    "            try:\n",
    "                cluster = ipp.Client(profile='{}{}'.format(self.cluster_job_name, cluster_id))\n",
    "            except ipp.error.TimeoutError:\n",
    "                attempt_ctr += 1\n",
    "            else:\n",
    "                self.cluster_pid_dict[cluster_id] = cluster[:].apply_async(os.getpid).get()\n",
    "                self.logger_status.info(('{}: \\t\\tCPU processes ready for action'\n",
    "                    ': {}').format(datetime.now(), self.cluster_pid_dict[cluster_id]))\n",
    "                return cluster\n",
    "            # if there is any other error other than TimeoutError, then the error will be raised\n",
    "            \n",
    "    def start_all_clusters(self):\n",
    "        \"\"\"Starts all the clusters specified and also writes updates to status.log\"\"\"\n",
    "        self.activate_logger()\n",
    "        self.logger_status.info('{}: Starting Multiple Cluster Engine'.format(datetime.now()))\n",
    "        for cluster_id, n_cpus in enumerate(self.n_cpus_list):\n",
    "            self.cluster_dict[cluster_id] = self.start_cluster(n_cpus, cluster_id)\n",
    "            self.load_balanced_view_dict[cluster_id] = self.cluster_dict[cluster_id].load_balanced_view()            \n",
    "        self.start_time = datetime.now()\n",
    "        self.logger_status.info('{}: All clusters started at {}'.format(datetime.now(), self.start_time))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        \n",
    "    def kill_cluster(self, cluster_id):\n",
    "        \"\"\"Given a cluster ID, kills that cluster and write update to status.log and\n",
    "        update cluster_indexes to know which clusters are remaining\"\"\"\n",
    "        self.logger_status.info((\"{}: \\tAttempting to kill cluster job {}'s {}th \"\n",
    "            \"cluster with CPU processes: {}\").format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, self.cluster_pid_dict[cluster_id]))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        # cluster.purge_everything() # sometimes this line takes forever\n",
    "        self.cluster_dict[cluster_id].close()\n",
    "        self.cluster_dict.pop(cluster_id)\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.logger_status.info('{}: \\t\\tCluster successfully killed'.format(datetime.now()))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        time.sleep(5) # hard-coded\n",
    "        \n",
    "    def kill_all_clusters(self):\n",
    "        \"\"\"Kills all clusters that are remaining and writes updates to status.log\"\"\"\n",
    "        self.end_time = datetime.now()\n",
    "        n_surviving_clusters = len(self.cluster_dict)\n",
    "        self.logger_status.info('{}: Killing all remaining clusters'.format(datetime.now()))\n",
    "        for cluster_id in sorted(self.cluster_dict):\n",
    "            self.kill_cluster(cluster_id)\n",
    "        self.logger_status.info('{}: All clusters have been killed'.format(datetime.now()))\n",
    "        self.logger_status.info('{}: Multiple Cluster Engine shut down at {}'.format(\n",
    "            datetime.now(), self.end_time))\n",
    "        self.logger_status.info((\"{}: Appears that {} files were successfully \"\n",
    "            \"processed using {} surviving clusters in {} minutes\").format(\n",
    "            datetime.now(), len(self.input_file_names) - self.n_unsuccessful_files, \n",
    "            n_surviving_clusters, (self.end_time - self.start_time).seconds / 60.0))\n",
    "        logging.shutdown()\n",
    "        \n",
    "    def early_kill_cluster(self, cluster_id):\n",
    "        \"\"\"Given a cluster ID, kills that cluster and writes which file that cluster\n",
    "        was working on to failure.log and updates status and RAM logs. An early cluster\n",
    "        kill happens when total RAM usage exceeds the limit set by the user\"\"\"\n",
    "        self.logger_failure.info((\"{}: Killing cluster job {}'s {}th cluster which \"\n",
    "            \"was processing file {} due to exceeding RAM limit\").format(\n",
    "            datetime.now(), self.cluster_job_name, cluster_id, \n",
    "            self.file_to_cluster_order_dict[cluster_id][-1]))\n",
    "        self.logger_ram_usage.info((\"{}: Killing cluster job {}'s {}th cluster due \"\n",
    "            \"to exceeding RAM limit\").format(datetime.now(), self.cluster_job_name, cluster_id))        \n",
    "        self.logger_status.info((\"{}: Killing cluster job {}'s {}th cluster with CPU \"\n",
    "            \"processes: {} due to exceeding RAM limit\").format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, self.cluster_pid_dict[cluster_id]))\n",
    "        self.cluster_dict[cluster_id].close()\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        self.cluster_dict.pop(cluster_id)\n",
    "        self.async_results_dict.pop(cluster_id)\n",
    "        self.n_unsuccessful_files += 1\n",
    "        \n",
    "    def kill_cluster_if_ram_limit_exceeded(self): \n",
    "        \"\"\"Checks if total RAM used exceeds limit set by user. If so, kill the \n",
    "        cluster that uses the most RAM and update some logs. Only kills at \n",
    "        max 1 cluster per method call\"\"\"\n",
    "        if sum(self.cluster_RAM_use_dict.values()) > self.ram_limit_in_GB:\n",
    "            cluster_id = sorted(self.cluster_RAM_use_dict, \n",
    "                                 key=self.cluster_RAM_use_dict.get, reverse=True)[0]\n",
    "            self.early_kill_cluster(cluster_id)\n",
    "            self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        if len(self.load_balanced_view_dict) == 0:\n",
    "            self.logger_failure.info((\"{}: All clusters have been killed prematurely \"\n",
    "                \"(probably due to exceeding RAM limit), so it would be a good idea\"\n",
    "                \" to determine which files, if any, successfully processed\"\n",
    "                 ).format(datetime.now()))\n",
    "            self.logger_status.info((\"{}: All clusters have been killed prematurely \"\n",
    "                \"(probably due to exceeding RAM limit)\").format(datetime.now()))\n",
    "            raise Exception('All clusters have been killed prematurely')\n",
    "    \n",
    "    def check_if_function_in_cluster_failed(self, cluster_id):\n",
    "        \"\"\"Given a cluster ID, checks if the most recent file (sent to\n",
    "        that cluster) successfully processed or not. If there was an error\n",
    "        in processing, then write the error to failure.log and remove\n",
    "        its async_result history\"\"\"\n",
    "        if self.async_results_dict[cluster_id] == []: # cluster just started, so it\n",
    "            return # doesn't have any files sent to the cluster yet\n",
    "        else:\n",
    "            exception = self.async_results_dict[cluster_id][-1].exception()\n",
    "            if exception:\n",
    "                self.logger_failure.info((\"{}: {}th cluster has error {} on \"\n",
    "                    \"file {}\").format(datetime.now(), cluster_id, exception.args[0], \n",
    "                    self.file_to_cluster_order_dict[cluster_id][-1]))\n",
    "                self.async_results_dict[cluster_id].pop()\n",
    "                self.n_unsuccessful_files += 1\n",
    "\n",
    "    def create_kwargs_dict_list(self, input_file_name, cluster_id, n_cpus):\n",
    "        \"\"\"Packages up the arguments into a dictionary to be sent to each cluster. \n",
    "        There are function arguments as well as cluster arguments (cluster ID and \n",
    "        number of CPUs in that cluster). If the cluster has n CPUs, then create\n",
    "        a list of n copies of this kwargs dictionary\"\"\"\n",
    "        function_kwargs_dict = copy.deepcopy(self.function_kwargs_dict)\n",
    "        function_kwargs_dict.update({'input_file_name': input_file_name,\n",
    "                                    'cluster_output_dir': self.cluster_output_dir,\n",
    "                                    'cluster_id': cluster_id,\n",
    "                                    'n_cpus': n_cpus})\n",
    "        function_kwargs_dict_list = []\n",
    "        for cpu_id in range(n_cpus):\n",
    "            function_kwargs_dict_list.append(copy.deepcopy(function_kwargs_dict))\n",
    "            function_kwargs_dict_list[cpu_id]['cpu_id'] = cpu_id\n",
    "        return function_kwargs_dict_list\n",
    "                \n",
    "    def run_clusters(self):\n",
    "        \"\"\"Iterates through all the files. You want to order your files from largest\n",
    "        to smallest. The reason is that the largest files (file0, file1, file2, etc)\n",
    "        will be sent to the largest cluster/cluster with the most CPUs (which \n",
    "        we will your first cluster). For each file, create a kwargs dictionary to \n",
    "        be sent to the cluster, write to status.log which file is going to which\n",
    "        cluster, and store the async_result in async_results_dict. You can also\n",
    "        inspect the order of files sent to which clusters by checking \n",
    "        file_to_cluster_order_dict after the engine has finished processing\n",
    "        all the files. For every wait_time_in_seconds, check if a cluster is\n",
    "        finished processing its current file and available to send the next\n",
    "        file. In addition, for every wait_time_in_seconds, check if RAM usage\n",
    "        exceeds limit set by user. If so, kill the cluster using the most RAM\"\"\"\n",
    "        small_file_ctr = 1 # effectively a dequeue scheme\n",
    "        big_file_ctr = 0\n",
    "        for ith_file in tqdm(range(len(self.input_file_names))):\n",
    "            while True:\n",
    "                time.sleep(self.wait_time_in_seconds)\n",
    "                self.profile_memory_for_all_clusters()\n",
    "                self.kill_cluster_if_ram_limit_exceeded()\n",
    "                jth_cluster = next(self.cluster_indexes)                \n",
    "                if (not self.async_results_dict[jth_cluster][-1:]\n",
    "                        or self.async_results_dict[jth_cluster][-1].done()): # check if cluster j is available                       \n",
    "                        self.clear_memory_on_cluster(jth_cluster)\n",
    "                    self.check_if_function_in_cluster_failed(jth_cluster) # check if previous file failed to process\n",
    "                    if jth_cluster == 0: # send large files to large cluster (which ALWAYS has id == 0)\n",
    "                        index = big_file_ctr\n",
    "                        big_file_ctr += 1\n",
    "                    else: # send small files to small clusters (which ALWAYS have id > 0)\n",
    "                        index = -small_file_ctr\n",
    "                        small_file_ctr += 1\n",
    "                                                                                   \n",
    "                    kwargs_dict_list = self.create_kwargs_dict_list(\n",
    "                        self.input_file_names[index],\n",
    "                        jth_cluster, \n",
    "                        len(self.cluster_dict[jth_cluster].ids))                    \n",
    "                    \n",
    "                    async_result = self.load_balanced_view_dict[jth_cluster].map_async(\n",
    "                        self.function_to_process, kwargs_dict_list)                                              \n",
    "                    self.async_results_dict[jth_cluster].append(async_result)\n",
    "                    self.file_to_cluster_order_dict[jth_cluster].append(self.input_file_names[index])\n",
    "                    # write status to file--it will only have start times, no end times\n",
    "                    self.logger_status.info((\"{}: {} is the {}th file and is sent to \"\n",
    "                        \"{}th cluster for processing\").format(datetime.now(),\n",
    "                        self.input_file_names[index], ith_file, jth_cluster))\n",
    "                    break # break out of inner loop to determine if other clusters are available\n",
    "\n",
    "        while not all(self.async_results_dict[jth_cluster][-1].done()\n",
    "                      for jth_cluster in self.async_results_dict): # wait for all clusters to finish\n",
    "            time.sleep(self.wait_time_in_seconds)\n",
    "            self.profile_memory_for_all_clusters()\n",
    "            self.kill_cluster_if_ram_limit_exceeded()\n",
    "                \n",
    "        cluster_set = set()\n",
    "        for jth_cluster in self.cluster_indexes:\n",
    "            if jth_cluster in cluster_set:\n",
    "                break\n",
    "            cluster_set.add(jth_cluster)\n",
    "            self.check_if_function_in_cluster_failed(jth_cluster) # check if last file failed to process\n",
    "        # async_results_dict; save to disk for later inspection? determine whether results takes too much RAM\n",
    "        \n",
    "    def main(self):\n",
    "        \"\"\"Runs the entire thing\"\"\"\n",
    "        self.create_cluster_output_dir()\n",
    "        self.start_all_clusters()\n",
    "        self.run_clusters()\n",
    "        self.kill_all_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Cluster Engine Usage\n",
    "Hello! Thank you for considering using the Multiple Cluster Engine.  \n",
    "Here's when you WANT to use this engine:\n",
    "* When you want to perform map-reduce on your data, which is constrained to 1 large machine (lots of RAM and CPUs) probably due to security concerns of migrating your data to other machines.  \n",
    "\n",
    "Do NOT use this engine:\n",
    "* If you have access to traditional map-reduce (like AWS Elastic Map Reduce), then regular map-reduce is probably faster if you throw lots of machines at it. If you have the money, just use regular map-reduce.\n",
    "* If you want to asychronously perform one task on lots of files that don't require map-reduce. For example, if you simply want to copy lots of big files from 1 directory to another asychronously at the same time, then just use the ipyparallel load balanced view. Basically you just want the multiprocessing module but don't want to write multiprocessing syntax because the multiprocessing module has some annoying limitations. Whatever you do using multiprocessing module can be done as easily or easier using ipyparallel. You can do something like this instead, which creates 1 cluster of 10 CPUs where each CPU does NOT talk to each other. :\n",
    "```python\n",
    "!ipcluster start --n=10 --profile=my_favorite_cluster --daemonize\n",
    "client = ipp.Client('my_favorite_cluster')\n",
    "load_balanced_view = client.load_balanced_view()\n",
    "async_result = load_balanced_view.map_async(your_function_here, your_files_here, other_argument_here_if_you_have_any)\n",
    "# with async_result, you can see how much time for each file has elapsed, whether if there was an\n",
    "# error, and other interesting statistics. Basically async_result records the history of what the cluster is doing\n",
    "!ipcluster stop --profile=my_favorite_cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Multiple Cluster Engine Works\n",
    "The Multiple Cluster Engine was used to extract features from large datasets on 1 machine. Here is the original paper describe how we approached the problem: https://docs.google.com/document/d/17-ItaCOXHbSqa2YmykpU4_PyC8--8bdMhEBGKjEURQo/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equally distribute the RFCs, however some RFCs have a crazy amount of invoices (Santander bank creates ~13% of all invoices!) \n",
    "practical advice to resolve this\n",
    "# figure out queue vs deque; deque is better\n",
    "# sorted from largest file to smallest, cluster for largest to smallestm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelled after MRJob, a library that makes map-reduce easier. https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/mapreduce/mapreduce-python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great tutorial about ipyparallel at https://github.com/DaanVanHauwermeiren/ipyparallel-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain what MCE does: map-reduce, vertical scaling instead of horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if necessary, recreate engine here if cluster shut down\n",
    "# write shell script for configuration and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest with a mapper/reducer? after each map/reducer step, use gc.collect()\n",
    "\n",
    "# MCE works on files. Hence, if you don't have any datafiles, then just create some empty files\n",
    "# SSD for parallel reading (not HDD); determine if you are IO constrained\n",
    "# RAM usage is heavier in Python 3 than Python 2; though Python 3 memory management is better\n",
    "# during function failure: benefit (error type will be saved to failure.log) and weakness \n",
    "#     (it doesn't say what line code failed at so you have to debug your function outside \n",
    "#     of the MCE instance. You have to debug as if it were just calling the function by itself on some data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
