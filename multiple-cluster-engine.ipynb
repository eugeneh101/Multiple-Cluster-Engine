{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Cluster Engine (MCE) Usage\n",
    "Hello! Thank you for considering using the Multiple Cluster Engine.  \n",
    "Here's when you WANT to use this engine:\n",
    "* When you want to perform map-reduce on your data, which is constrained to 1 large machine (lots of RAM and CPUs) probably due to security concerns of migrating your data to other machines.  \n",
    "\n",
    "Do NOT use this engine:\n",
    "* If you have access to traditional map-reduce (like AWS Elastic Map Reduce) AND your dataset is incredibly massive, then regular map-reduce is probably faster if you throw lots of machines at it. If you have the money, just use regular map-reduce.\n",
    "* If you want to asychronously perform one task on lots of files that don't require map-reduce. For example, if you simply want to copy lots of big files from 1 directory to another asychronously at the same time, then just use the ipyparallel load balanced view. Basically you just want the multiprocessing module but don't want to write multiprocessing syntax because the multiprocessing module has some annoying limitations. Whatever you do using multiprocessing module can be done as easily or easier using ipyparallel. You can do something like this instead, which creates 1 cluster of 10 CPUs where each CPU does NOT talk to each other. :\n",
    "```python\n",
    "!ipcluster start --n=10 --profile=my_favorite_cluster --daemonize\n",
    "time.sleep(10) # it takes a few seconds for the bash command to start up the clusters\n",
    "client = ipp.Client('my_favorite_cluster')\n",
    "load_balanced_view = client.load_balanced_view()\n",
    "async_result = load_balanced_view.map_async(your_function_here, your_files_here, other_argument_here_if_you_have_any)\n",
    "# with async_result, you can see how much time for each file has elapsed, whether if there was an\n",
    "# error, and other interesting statistics. Basically async_result records the history of what the cluster is doing\n",
    "!ipcluster stop --profile=my_favorite_cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivtion for Using the Multiple Cluster Engine\n",
    "The motivation is that the MCE is quite fast for large datasets, as it performs all calculations in RAM as opposed to a traditional map-reduce job which after the mapping steps, saves data to disk, sorts, shuffles, and load back to RAM. Traditional map-reduce is slower in that the reducing step cannot start until the entirety of the mapping step is completed. Any operations involving saving to disk instead of in RAM will be much slower. The MCE overcomes this hurdle in that each CPU in a cluster is effectively isolated from each other and performs BOTH mapping and reducing steps and thus isn't dependent on the completion of other CPUs in its cluster.  \n",
    "__CAVEAT:__ With enough money for many machines and with a dataset large enough, traditional map-reduce can still be faster. Basically, 20 small machines might be faster than 1 large machine--and it maybe be possible that 20 small machines might be cheaper if runtime is significantly reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Multiple Cluster Engine Works\n",
    "The Multiple Cluster Engine allows vertical scaling on a large machine in place of horizontal scaling where you use multiple small machines. Here is the original paper called \"Multi-Cluster Engine: A Pragmatic Approach to Big Data\"  (https://docs.google.com/document/d/17-ItaCOXHbSqa2YmykpU4_PyC8--8bdMhEBGKjEURQo/edit), describing our approached of performing map-reduce to extract features from large datasets.  \n",
    "A high-level explanation of MCE: Each CPU is effectively a complete map-reduce job on its PARTITION of the user IDs. Each cluster composed of CPUs extracts features for ALL the user IDs for the input file. Multiple clusters can extract features for multiple files simultaneously.  \n",
    "\n",
    "Suppose we create a cluster with 7 CPUs (labeled CPU0, CPU1, ..., CPU6), each CPU will read all the data for 1 month. \n",
    "* Mapping stage: For each line, each CPU hashes the user ID and performs mod 7 and determines if that user ID belongs to that CPU (meaning the remainder == CPU ID). Using this scheme, we can guarantee that for a specific user ID, all the invoice data for that month will go to only 1 CPU. For each CPU, the data it remembers will be stored in a defaultdict where the key is the user ID and the value is a list of invoice data. Use the defaultdict(list), so you can simply append entries for each user ID.\n",
    "* Reducing stage: create a pandas dataframe for only 1 user ID's data at a time and calculate the features you care about. After getting the results, store the results into a separate dictionary and immediately dump the raw data in order to reduce RAM usage. Once you iterate through all the user IDs, then save the results to file. You're done!\n",
    "\n",
    "1 cluster is working on 1 file at 1 time. MCE abstracts this further where you create multiple clusters, so you can extract from multiple files files asychronously at the same time. The next file is sent to the next available cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Advice about the Multiple Cluster Engine\n",
    "* MCE setup assumes that the number of CPUs for clusters are ordered from most to fewest (i.e. cluster0: 5 CPUs, cluster1: 4 CPUs, cluster2: 3 CPUs) and the list of input files are ordered from largest to smallest (i.e. 60 GB, 40 GB, 20 GB). The reason is that in MCE, cluster0 receives the leftmost/largest files in the list, and all other clusters receive files from the rightmost/smallest files; when MCE finishes all the files, the last file to be processed will actually be somewhere in the middle of the input list. The idea for the double-ended queue is to load balance the RAM. A counter-factual example is that if you queued up files from smallest to largest, the files at the end would take too much RAM as large files would be loaded into RAM simultaneously and ultimately crashing the RAM. Luckily, the MCE constantly profiles RAM and will kill the cluster using the most RAM after exceeding the user's preset RAM limit, so this will never happen. Nonetheless, killing a cluster means you are operating with fewer clusters.  \n",
    "* Here is the tradeoff you experience to optimize the objective function (reducing run time): CPU, RAM, and storage.\n",
    "* More CPUs in a single cluster means that for 1 file the reducing step will be faster (fewer user IDs per CPU) but the mapping step will still take the same time (every CPU will read the entire file). However, more CPUs in 1 cluster means you can't put the CPUs on other clusters. More clusters means processing more files simultaneously. However, more clusters also mean using more RAM as each file will be loaded into RAM.  \n",
    "* You don't want more CPUs in the MCE than you have logical CPUs. For example, if you 24 CPUs on your VM, then the MCE should have at max 22 or 23 CPUs--leave 1 or 2 for remaining processes like OS. \n",
    "* Althought it may seem inefficient to have every CPU in 1 cluster read the same file independently, it turns out that map-reduce is not constrained by read time nor is it constrained by loading the data into RAM into the defaultdict. The bulk of the time is actually performing the calculations. If you are concerned about read time and performing unnecessary hashes on user ID, then you can write a Multiple Cluster job that first partitions the data based on user ID and then write that to disk. Basically, you do a 1-time map-reduce job where the mapping step collects all the data from a specific user ID and the reducing step is just writing each user ID data block back to file. The benefit is the data is now effectively already shuffled and sorted so you don't need to load in all the data any more. For future map-reduce jobs, you can just load in 1 user ID data block at a time, perform calculation, append to some results file, immediately delete the raw data from RAM, and repeat with next user ID. The tradeoff is that you use more SSD space, which is not cheap. Of course, a further optimization is writing to file how long the mapping (reading data) and reducing (performing calculations) steps take, so you know how many CPUs to put on a cluster. \n",
    "* The hashing of user ID is to randomly distribute user IDs to different CPUs. An interesting property is that the number of invoices for a user ID exhibits a power law distribution. Ideally, each CPU will have an equal amount of user IDs that have lots of invoices. The only downside is that there are a handful of user IDs that have incredible amounts of invoices: Santander bank has ~13% of all invoices. Hence, when a CPU is assigned to Santander bank, then it is the slowest CPU as it probably has to load more into its RAM during mapping and ultimately create a very large dataframe for reducing. Hence, in a cluster, you will see that this CPU slows down the entire cluster, as the cluster cannot move onto the next file until all the results for each user ID is completed for its input file. The wait time when other CPUs lay idle while 1 CPU is processing Santander is not trivial. In my opinion, this problem is the biggest thing that slows down the entire MCE job. Ideally you want for each CPU in a cluster to finish processing its user IDs around the same time. A resolution is to partition Santander and other known user IDs with inordinately large number of invoices from the rest of the dataset but __ONLY__ if the MCE job is too slow for you. Zen of Python: premature optimization is the root of all evil. \n",
    "* After the mapping step and before the reducing step, run gc.collect() to release some memory. It helps sometimes and is quick to run.\n",
    "* The transition between Python 3 and 2 isn't very large: mostly print function vs statement. The main issue will be in the invoice data where the text are encoded with Window's encoding (cp-1252) (which had problems for reasons I forget), so we had to convert to UTF or ISO Latin 8859. Hence, for the function you put into MCE, you just have to be careful with Python 3's string encoding which will be UTF, which you'll have to figure out how to play nice with cp-1252. A practical example are Spanish characters with tildes or accents that, at least in Python 2, caused issues. (Perhaps, it was that cp-1252 strings were not hashable).  \n",
    "* From observation, it appears that you don't want to get too close to 100% RAM utilization as all operations take significantly longer. I am not sure why this is happening. My guess is try to keep RAM utilization peaked at 80%; just a guess. \n",
    "* You want to use SSD when you use this engine. The reason is HDD is significantly slower to read data, and it probably can't read from different files simultaneously very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool Built-In Features:\n",
    "* Profiles RAM for each cluster (and sums total to get total RAM for entire MCE) every ```wait_time_in_seconds``` and saves to ```ram_usage.log```. If RAM of all clusters exceed user's preset RAM limit, then MCE will automatically kill the cluster using the most RAM (in practice, probably cluster0) AND write to ```failure.log``` which cluster got killed and which file it was working on at the time. \n",
    "* If the function put into MCE failed (like typeError, DivisionByZero, etc.), then that failure exception will be written into ```failure.log``` along with which cluster on which file. This is useful because the invoice data is frankly not always standardized--sometimes the delimiters change, sometimes the headers change, etc. Basically, the ```failure.log``` file tells you what went wrong on which file. However, to debug your function, you have to have to run the function without MCE but pass in arguments that MCE would have passed in, in order to reproduce the error as well as determine which line the code failed on. \n",
    "* After setting up MCE, the main() method will create the final output directory, start all the clusters, run the clusters where files are sent to the clusters, and then killing all the clusters at the end. Ideally everything is self contained. Occasionally, if there is an error and MCE stops running, you have to go to ```htop``` and find if there are any cluster processes still alive and manually kill them using ```ipcluster stop --profile=cluster_name_here```\n",
    "* Each time you run MCE, MCE will create a new final output directory. Hence, you can safely run the same code multiple times and not overwrite your previous output. \n",
    "* Written and tested to be Python 3 and 2 compliant. In terms of RAM, Python 2 is a bit greedy in that it doesn't release all unused memory to the OS. The benefit for Python 3 is that it releases more (but not all) unused memory back to the OS. For example, if you create a large list in Python 2 and delete it, the amount of free RAM is lower than before. In Python 3 if you create a large list and delete it, the amount of free RAM is higher than that of Python 2. This is important in that when processing large files in MCE, each CPU will have some RAM attached after its finished processing its file, despite having no variables in its namespace. You want to have the smallest RAM footprint when each CPU finishes what its doing, so you can have more clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area for Future Improvements\n",
    "* Allow queue instead of double-ended queue for input file lists. The motivation is that you actually want the results from some input files before others in a predictable way. This should be actually quite easy.\n",
    "* Currently, if a cluster is running smoothly, it will never kill itself. Suppose RAM accumulates because of a memory leak, then implement a method to kill and restart the cluster. This should be easy. Personally, I don't know if memory leaks will be a big issue, so I didn't build this feature.\n",
    "* If a cluster is killed, then perhaps in the future, turn that cluster back on--basically self-regenertion. This will be quite difficult for lots of reasons. For practical purposes, I do not see this as a feature, as there can be many things that can go wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* My quick, 5-minute tutorial of ipyparallel to cover all the basics: https://github.com/eugeneh101/ipyparallel-vs-MRJob/blob/master/ipyparallel_tutorial.ipynb  \n",
    "* A more extensive coverage of ipyparallel at https://github.com/DaanVanHauwermeiren/ipyparallel-tutorial\n",
    "* I drew inspiration for MCE from MRJob, a library that makes map-reduce easier using OOP. Here's my MRJob example done in less than 10 lines of code: https://github.com/eugeneh101/ipyparallel-vs-MRJob/blob/master/ipyparallel_example_vs_mrjob.ipynb\n",
    "* This MRJob tutorial shows how you can deploy Map-reduce on AWS EMR using very little code. The only caveat is I was able to use MRJob locally but never figured out how set up the AWS EMR version correctly. https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/mapreduce/mapreduce-python.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write shell script for configuration and installation\n",
    "# unittest with a mapper/reducer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just use this Jupyter Notebooks magic function (%%file) to save the code to the .py file. Basically, the code cell below functioned as my Sublime editor (with syntax highlighting and tab completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting multiple_cluster_engine.py\n"
     ]
    }
   ],
   "source": [
    "%%file multiple_cluster_engine.py\n",
    "import ipyparallel as ipp\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import logging # logging can create duplicate entries if you don't reload logging\n",
    "try:\n",
    "    from importlib import reload # Python 3\n",
    "except: # Python 2 reload is a builtin\n",
    "    pass\n",
    "\n",
    "class MultipleClusterEngine(object):\n",
    "    def __init__(self, \n",
    "                 cluster_job_name, \n",
    "                 n_cpus_list, \n",
    "                 ram_limit_in_GB,\n",
    "                 wait_time_in_seconds,\n",
    "                 input_file_names,\n",
    "                 output_parent_dir,\n",
    "                 function_to_process,\n",
    "                 function_kwargs_dict): # always put function args in a dictionary\n",
    "        reload(logging)\n",
    "        self.cluster_job_name = cluster_job_name\n",
    "        self.n_cpus_list = n_cpus_list\n",
    "        self.ram_limit_in_GB = ram_limit_in_GB\n",
    "        self.wait_time_in_seconds = wait_time_in_seconds\n",
    "        self.output_parent_dir = output_parent_dir\n",
    "        self.input_file_names = input_file_names\n",
    "        self.function_to_process = lambda kwargs: function_to_process(**kwargs)\n",
    "        self.function_kwargs_dict = function_kwargs_dict\n",
    "        \n",
    "        assert cluster_job_name, \"Needs cluster name\"\n",
    "        assert len(n_cpus_list) > 0, \"Needs the number of CPUs per cluster\"\n",
    "        assert os.path.isdir(self.output_parent_dir), \"Output directory doesn't exist\"\n",
    "        assert len(self.input_file_names) > 0, \"Need input files\"\n",
    "\n",
    "        # used by engine\n",
    "        self.cluster_dict = {}\n",
    "        self.load_balanced_view_dict = {}\n",
    "        self.async_results_dict = defaultdict(list) # collects all the async_results\n",
    "        self.file_to_cluster_order_dict = defaultdict(list) # remembers which file is sent to which cluster\n",
    "        self.cluster_indexes = None\n",
    "        self.logger_status = None\n",
    "        self.logger_failure = None\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.cluster_output_dir = None\n",
    "        self.cluster_RAM_use_dict = {}\n",
    "        self.cluster_pid_dict = {}\n",
    "        self.n_unsuccessful_files = 0\n",
    "\n",
    "    def create_cluster_output_dir(self):\n",
    "        \"\"\"Creates the folder which all the results will be saved to based on\n",
    "        the output_parent_dir and the cluster job name and an incremented number\"\"\"\n",
    "        subdirs = [name for name in os.listdir(self.output_parent_dir) if \n",
    "                   os.path.isdir(os.path.join(self.output_parent_dir, name))]\n",
    "        existing_results_dir = []\n",
    "        for subdir in subdirs:\n",
    "            try:\n",
    "                existing_results_dir.append(int(subdir.strip(self.cluster_job_name)))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        dir_index = max(existing_results_dir) + 1 if existing_results_dir else 0\n",
    "        self.cluster_output_dir = os.path.join(self.output_parent_dir, \n",
    "                                               self.cluster_job_name + str(dir_index))\n",
    "        os.makedirs(self.cluster_output_dir)\n",
    "                \n",
    "    def create_logger(self, logger_name, log_file):\n",
    "        \"\"\"Create logger objects\"\"\"\n",
    "        l = logging.getLogger(logger_name)\n",
    "        fileHandler = logging.FileHandler(log_file)\n",
    "        l.addHandler(fileHandler)\n",
    "        l.setLevel(logging.INFO)\n",
    "    \n",
    "    def activate_logger(self):\n",
    "        \"\"\"Create a logger for status, failure, and RAM usage updates\"\"\"\n",
    "        self.create_logger('status', os.path.join(self.cluster_output_dir, \"status.log\"))\n",
    "        self.create_logger('failure', os.path.join(self.cluster_output_dir, \"failure.log\"))\n",
    "        self.create_logger('ram_usage', os.path.join(self.cluster_output_dir, \"ram_usage.log\"))\n",
    "        self.logger_status = logging.getLogger('status')\n",
    "        self.logger_status.propagate = False\n",
    "        self.logger_failure = logging.getLogger('failure')\n",
    "        self.logger_failure.propagate = False\n",
    "        self.logger_ram_usage = logging.getLogger('ram_usage')\n",
    "        self.logger_ram_usage.propagate = False\n",
    "        \n",
    "    def profile_memory_for_cluster(self, cluster_id): # RAM in GB\n",
    "        \"\"\"Given a cluster ID, determines how much RAM that cluster is using,\n",
    "        which is just the sum of the RAM attached the CPUs in the cluster\"\"\"\n",
    "        return sum(psutil.Process(pid).memory_info().rss for \n",
    "                   pid in self.cluster_pid_dict[cluster_id]) / 1e9\n",
    "        \n",
    "    def profile_memory_for_all_clusters(self):\n",
    "        \"\"\"Determines the RAM for all the clusters\"\"\"\n",
    "        self.cluster_RAM_use_dict.clear()\n",
    "        for jth_cluster in sorted(self.load_balanced_view_dict):\n",
    "            self.cluster_RAM_use_dict[jth_cluster] = self.profile_memory_for_cluster(jth_cluster)           \n",
    "            self.logger_ram_usage.info('{}: {}th cluster uses {} GB of RAM'.format(\n",
    "                datetime.now(), jth_cluster, self.cluster_RAM_use_dict[jth_cluster]))\n",
    "        self.logger_ram_usage.info('{}: All clusters use {} GB of RAM'.format(\n",
    "                datetime.now(), sum(self.cluster_RAM_use_dict.values())))\n",
    "        \n",
    "    def clear_memory_on_cluster(self, cluster_id): # not as effective as imagined\n",
    "        \"\"\"Ideally clears out the RAM when a cluster successfully processes a file.\n",
    "        In practice, it is hard to say how effective this is in clearing the RAM.\n",
    "        Fortunately, running this method is very fast\"\"\"\n",
    "        import gc\n",
    "        self.cluster_dict[cluster_id][:].apply_async(gc.collect)\n",
    "        \n",
    "    def start_cluster(self, n_cpus, cluster_id):\n",
    "        \"\"\"Given a cluster ID, start the cluster with that ID and then attach\n",
    "        to the cluster and then store the CPU PIDs cluster_pid_dict and then\n",
    "        log it in the status.log file\n",
    "        \"\"\"\n",
    "        self.logger_status.info(\"{}: \\tAttempting to start cluster job \"\n",
    "            \"{}'s {}th cluster with {} CPUs\".format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, n_cpus))\n",
    "        os.system(\"ipcluster start --n={} --profile={}{} --daemonize\".format(\n",
    "            n_cpus, self.cluster_job_name, cluster_id)) # should deprecate to use a safer OS call\n",
    "\n",
    "        attempt_ctr = 0 \n",
    "        while attempt_ctr < 3: # Attempt to connect to client 3 times\n",
    "            time.sleep(10) # hard coded\n",
    "            try:\n",
    "                cluster = ipp.Client(profile='{}{}'.format(self.cluster_job_name, cluster_id))\n",
    "            except ipp.error.TimeoutError:\n",
    "                attempt_ctr += 1\n",
    "            else:\n",
    "                self.cluster_pid_dict[cluster_id] = cluster[:].apply_async(os.getpid).get()\n",
    "                self.logger_status.info(('{}: \\t\\tCPU processes ready for action'\n",
    "                    ': {}').format(datetime.now(), self.cluster_pid_dict[cluster_id]))\n",
    "                return cluster\n",
    "            # if there is any other error other than TimeoutError, then the error will be raised\n",
    "            \n",
    "    def start_all_clusters(self):\n",
    "        \"\"\"Starts all the clusters specified and also writes updates to status.log\"\"\"\n",
    "        self.activate_logger()\n",
    "        self.logger_status.info('{}: Starting Multiple Cluster Engine'.format(datetime.now()))\n",
    "        for cluster_id, n_cpus in enumerate(self.n_cpus_list):\n",
    "            self.cluster_dict[cluster_id] = self.start_cluster(n_cpus, cluster_id)\n",
    "            self.load_balanced_view_dict[cluster_id] = self.cluster_dict[cluster_id].load_balanced_view()            \n",
    "        self.start_time = datetime.now()\n",
    "        self.logger_status.info('{}: All clusters started at {}'.format(datetime.now(), self.start_time))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        \n",
    "    def kill_cluster(self, cluster_id):\n",
    "        \"\"\"Given a cluster ID, kills that cluster and write update to status.log and\n",
    "        update cluster_indexes to know which clusters are remaining\"\"\"\n",
    "        self.logger_status.info((\"{}: \\tAttempting to kill cluster job {}'s {}th \"\n",
    "            \"cluster with CPU processes: {}\").format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, self.cluster_pid_dict[cluster_id]))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        # cluster.purge_everything() # sometimes this line takes forever\n",
    "        self.cluster_dict[cluster_id].close()\n",
    "        self.cluster_dict.pop(cluster_id)\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.logger_status.info('{}: \\t\\tCluster successfully killed'.format(datetime.now()))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        time.sleep(5) # hard-coded\n",
    "        \n",
    "    def kill_all_clusters(self):\n",
    "        \"\"\"Kills all clusters that are remaining and writes updates to status.log\"\"\"\n",
    "        self.end_time = datetime.now()\n",
    "        n_surviving_clusters = len(self.cluster_dict)\n",
    "        self.logger_status.info('{}: Killing all remaining clusters'.format(datetime.now()))\n",
    "        for cluster_id in sorted(self.cluster_dict):\n",
    "            self.kill_cluster(cluster_id)\n",
    "        self.logger_status.info('{}: All clusters have been killed'.format(datetime.now()))\n",
    "        self.logger_status.info('{}: Multiple Cluster Engine shut down at {}'.format(\n",
    "            datetime.now(), self.end_time))\n",
    "        self.logger_status.info((\"{}: Appears that {} files were successfully \"\n",
    "            \"processed using {} surviving clusters in {} minutes\").format(\n",
    "            datetime.now(), len(self.input_file_names) - self.n_unsuccessful_files, \n",
    "            n_surviving_clusters, (self.end_time - self.start_time).seconds / 60.0))\n",
    "        logging.shutdown()\n",
    "        \n",
    "    def early_kill_cluster(self, cluster_id):\n",
    "        \"\"\"Given a cluster ID, kills that cluster and writes which file that cluster\n",
    "        was working on to failure.log and updates status and RAM logs. An early cluster\n",
    "        kill happens when total RAM usage exceeds the limit set by the user\"\"\"\n",
    "        self.logger_failure.info((\"{}: Killing cluster job {}'s {}th cluster which \"\n",
    "            \"was processing file {} due to exceeding RAM limit\").format(\n",
    "            datetime.now(), self.cluster_job_name, cluster_id, \n",
    "            self.file_to_cluster_order_dict[cluster_id][-1]))\n",
    "        self.logger_ram_usage.info((\"{}: Killing cluster job {}'s {}th cluster due \"\n",
    "            \"to exceeding RAM limit\").format(datetime.now(), self.cluster_job_name, cluster_id))        \n",
    "        self.logger_status.info((\"{}: Killing cluster job {}'s {}th cluster with CPU \"\n",
    "            \"processes: {} due to exceeding RAM limit\").format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, self.cluster_pid_dict[cluster_id]))\n",
    "        self.cluster_dict[cluster_id].close()\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        self.cluster_dict.pop(cluster_id)\n",
    "        self.async_results_dict.pop(cluster_id)\n",
    "        self.n_unsuccessful_files += 1\n",
    "        \n",
    "    def kill_cluster_if_ram_limit_exceeded(self): \n",
    "        \"\"\"Checks if total RAM used exceeds limit set by user. If so, kill the \n",
    "        cluster that uses the most RAM and update some logs. Only kills at \n",
    "        max 1 cluster per method call\"\"\"\n",
    "        if sum(self.cluster_RAM_use_dict.values()) > self.ram_limit_in_GB:\n",
    "            cluster_id = sorted(self.cluster_RAM_use_dict, \n",
    "                                 key=self.cluster_RAM_use_dict.get, reverse=True)[0]\n",
    "            self.early_kill_cluster(cluster_id)\n",
    "            self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        if len(self.load_balanced_view_dict) == 0:\n",
    "            self.logger_failure.info((\"{}: All clusters have been killed prematurely \"\n",
    "                \"(probably due to exceeding RAM limit), so it would be a good idea\"\n",
    "                \" to determine which files, if any, successfully processed\"\n",
    "                 ).format(datetime.now()))\n",
    "            self.logger_status.info((\"{}: All clusters have been killed prematurely \"\n",
    "                \"(probably due to exceeding RAM limit)\").format(datetime.now()))\n",
    "            raise Exception('All clusters have been killed prematurely')\n",
    "    \n",
    "    def check_if_function_in_cluster_failed(self, cluster_id):\n",
    "        \"\"\"Given a cluster ID, checks if the most recent file (sent to\n",
    "        that cluster) successfully processed or not. If there was an error\n",
    "        in processing, then write the error to failure.log and remove\n",
    "        its async_result history\"\"\"\n",
    "        if self.async_results_dict[cluster_id] == []: # cluster just started, so it\n",
    "            return # doesn't have any files sent to the cluster yet\n",
    "        else:\n",
    "            exception = self.async_results_dict[cluster_id][-1].exception()\n",
    "            if exception:\n",
    "                self.logger_failure.info((\"{}: {}th cluster has error {} on \"\n",
    "                    \"file {}\").format(datetime.now(), cluster_id, exception.args[0], \n",
    "                    self.file_to_cluster_order_dict[cluster_id][-1]))\n",
    "                self.async_results_dict[cluster_id].pop()\n",
    "                self.n_unsuccessful_files += 1\n",
    "\n",
    "    def create_kwargs_dict_list(self, input_file_name, cluster_id, n_cpus):\n",
    "        \"\"\"Packages up the arguments into a dictionary to be sent to each cluster. \n",
    "        There are function arguments as well as cluster arguments (cluster ID and \n",
    "        number of CPUs in that cluster). If the cluster has n CPUs, then create\n",
    "        a list of n copies of this kwargs dictionary\"\"\"\n",
    "        function_kwargs_dict = copy.deepcopy(self.function_kwargs_dict)\n",
    "        function_kwargs_dict.update({'input_file_name': input_file_name,\n",
    "                                    'cluster_output_dir': self.cluster_output_dir,\n",
    "                                    'cluster_id': cluster_id,\n",
    "                                    'n_cpus': n_cpus})\n",
    "        function_kwargs_dict_list = []\n",
    "        for cpu_id in range(n_cpus):\n",
    "            function_kwargs_dict_list.append(copy.deepcopy(function_kwargs_dict))\n",
    "            function_kwargs_dict_list[cpu_id]['cpu_id'] = cpu_id\n",
    "        return function_kwargs_dict_list\n",
    "                \n",
    "    def run_clusters(self):\n",
    "        \"\"\"Iterates through all the files. You want to order your files from largest\n",
    "        to smallest. The reason is that the largest files (file0, file1, file2, etc)\n",
    "        will be sent to the largest cluster/cluster with the most CPUs (which \n",
    "        we will your first cluster). For each file, create a kwargs dictionary to \n",
    "        be sent to the cluster, write to status.log which file is going to which\n",
    "        cluster, and store the async_result in async_results_dict. You can also\n",
    "        inspect the order of files sent to which clusters by checking \n",
    "        file_to_cluster_order_dict after the engine has finished processing\n",
    "        all the files. For every wait_time_in_seconds, check if a cluster is\n",
    "        finished processing its current file and available to send the next\n",
    "        file. In addition, for every wait_time_in_seconds, check if RAM usage\n",
    "        exceeds limit set by user. If so, kill the cluster using the most RAM\"\"\"\n",
    "        small_file_ctr = 1 # effectively a dequeue scheme\n",
    "        big_file_ctr = 0\n",
    "        for ith_file in tqdm(range(len(self.input_file_names))):\n",
    "            while True:\n",
    "                time.sleep(self.wait_time_in_seconds)\n",
    "                self.profile_memory_for_all_clusters()\n",
    "                self.kill_cluster_if_ram_limit_exceeded()\n",
    "                jth_cluster = next(self.cluster_indexes)                \n",
    "                if (not self.async_results_dict[jth_cluster][-1:]\n",
    "                        or self.async_results_dict[jth_cluster][-1].done()): # check if cluster j is available                       \n",
    "                        self.clear_memory_on_cluster(jth_cluster)\n",
    "                    self.check_if_function_in_cluster_failed(jth_cluster) # check if previous file failed to process\n",
    "                    if jth_cluster == 0: # send large files to large cluster (which ALWAYS has id == 0)\n",
    "                        index = big_file_ctr\n",
    "                        big_file_ctr += 1\n",
    "                    else: # send small files to small clusters (which ALWAYS have id > 0)\n",
    "                        index = -small_file_ctr\n",
    "                        small_file_ctr += 1\n",
    "                                                                                   \n",
    "                    kwargs_dict_list = self.create_kwargs_dict_list(\n",
    "                        self.input_file_names[index],\n",
    "                        jth_cluster, \n",
    "                        len(self.cluster_dict[jth_cluster].ids))                    \n",
    "                    \n",
    "                    async_result = self.load_balanced_view_dict[jth_cluster].map_async(\n",
    "                        self.function_to_process, kwargs_dict_list)                                              \n",
    "                    self.async_results_dict[jth_cluster].append(async_result)\n",
    "                    self.file_to_cluster_order_dict[jth_cluster].append(self.input_file_names[index])\n",
    "                    # write status to file--it will only have start times, no end times\n",
    "                    self.logger_status.info((\"{}: {} is the {}th file and is sent to \"\n",
    "                        \"{}th cluster for processing\").format(datetime.now(),\n",
    "                        self.input_file_names[index], ith_file, jth_cluster))\n",
    "                    break # break out of inner loop to determine if other clusters are available\n",
    "\n",
    "        while not all(self.async_results_dict[jth_cluster][-1].done()\n",
    "                      for jth_cluster in self.async_results_dict): # wait for all clusters to finish\n",
    "            time.sleep(self.wait_time_in_seconds)\n",
    "            self.profile_memory_for_all_clusters()\n",
    "            self.kill_cluster_if_ram_limit_exceeded()\n",
    "                \n",
    "        cluster_set = set()\n",
    "        for jth_cluster in self.cluster_indexes:\n",
    "            if jth_cluster in cluster_set:\n",
    "                break\n",
    "            cluster_set.add(jth_cluster)\n",
    "            self.check_if_function_in_cluster_failed(jth_cluster) # check if last file failed to process\n",
    "        # async_results_dict; save to disk for later inspection? determine whether results takes too much RAM\n",
    "        \n",
    "    def main(self):\n",
    "        \"\"\"Runs the entire thing\"\"\"\n",
    "        self.create_cluster_output_dir()\n",
    "        self.start_all_clusters()\n",
    "        self.run_clusters()\n",
    "        self.kill_all_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
