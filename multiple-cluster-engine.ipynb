{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11111111111111111111111111111111111111111111111111111111111111111111111111111111 # 80 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting multiple_cluster_engine.py\n"
     ]
    }
   ],
   "source": [
    "%%file multiple_cluster_engine.py\n",
    "import ipyparallel as ipp\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import logging # logging can create duplicate entries if you don't reload logging\n",
    "try:\n",
    "    from importlib import reload # Python 3\n",
    "except: # Python 2 reload is a builtin\n",
    "    pass\n",
    "\n",
    "class MultipleClusterEngine(object):\n",
    "    def __init__(self, \n",
    "                 cluster_job_name, \n",
    "                 n_cpus_list, \n",
    "                 ram_limit_in_GB,\n",
    "                 wait_time_in_seconds,\n",
    "                 input_file_names,\n",
    "                 output_parent_dir,\n",
    "                 function_to_process,\n",
    "                 function_kwargs_dict): # always put function args in a dictionary\n",
    "        reload(logging)\n",
    "        self.cluster_job_name = cluster_job_name\n",
    "        self.n_cpus_list = n_cpus_list\n",
    "        self.ram_limit_in_GB = ram_limit_in_GB\n",
    "        self.wait_time_in_seconds = wait_time_in_seconds\n",
    "        self.output_parent_dir = output_parent_dir\n",
    "        self.input_file_names = input_file_names\n",
    "        self.function_to_process = lambda kwargs: function_to_process(**kwargs)\n",
    "        self.function_kwargs_dict = function_kwargs_dict\n",
    "        \n",
    "        assert cluster_job_name, \"Needs cluster name\"\n",
    "        assert len(n_cpus_list) > 0, \"Needs the number of CPUs per cluster\"\n",
    "        assert os.path.isdir(self.output_parent_dir), \"Output directory doesn't exist\"\n",
    "        assert len(self.input_file_names) > 0, \"Need input files\"\n",
    "\n",
    "        # used by engine\n",
    "        self.cluster_dict = {}\n",
    "        self.load_balanced_view_dict = {}\n",
    "        self.async_results_dict = defaultdict(list) # collects all the async_results\n",
    "        self.file_to_cluster_order_dict = defaultdict(list) # remembers which file is sent to which cluster\n",
    "        self.cluster_indexes = None\n",
    "        self.logger_status = None\n",
    "        self.logger_failure = None\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.cluster_output_dir = None\n",
    "        self.cluster_RAM_use_dict = {}\n",
    "        self.cluster_pid_dict = {}\n",
    "\n",
    "    def create_cluster_output_dir(self):\n",
    "        subdirs = [name for name in os.listdir(self.output_parent_dir) if \n",
    "                   os.path.isdir(os.path.join(self.output_parent_dir, name))]\n",
    "        existing_results_dir = []\n",
    "        for subdir in subdirs:\n",
    "            try:\n",
    "                existing_results_dir.append(int(subdir.strip(self.cluster_job_name)))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        dir_index = max(existing_results_dir) + 1 if existing_results_dir else 0\n",
    "        self.cluster_output_dir = os.path.join(self.output_parent_dir, \n",
    "                                               self.cluster_job_name + str(dir_index))\n",
    "        os.makedirs(self.cluster_output_dir)\n",
    "                \n",
    "    def create_logger(self, logger_name, log_file):\n",
    "        l = logging.getLogger(logger_name)\n",
    "        fileHandler = logging.FileHandler(log_file)\n",
    "        l.addHandler(fileHandler)\n",
    "        l.setLevel(logging.INFO)\n",
    "    \n",
    "    def activate_logger(self):\n",
    "        self.create_logger('status', os.path.join(self.cluster_output_dir, \"status.log\"))\n",
    "        self.create_logger('failure', os.path.join(self.cluster_output_dir, \"failure.log\"))\n",
    "        self.create_logger('ram_usage', os.path.join(self.cluster_output_dir, \"ram_usage.log\"))\n",
    "        self.logger_status = logging.getLogger('status')\n",
    "        self.logger_status.propagate = False\n",
    "        self.logger_failure = logging.getLogger('failure')\n",
    "        self.logger_failure.propagate = False\n",
    "        self.logger_ram_usage = logging.getLogger('ram_usage')\n",
    "        self.logger_ram_usage.propagate = False\n",
    "        \n",
    "    def profile_memory_for_cluster(self, cluster_id): # RAM in GB\n",
    "        return sum(psutil.Process(pid).memory_info().rss for \n",
    "                   pid in self.cluster_pid_dict[cluster_id]) / 1e9\n",
    "        \n",
    "    def profile_memory_for_all_clusters(self):\n",
    "        self.cluster_RAM_use_dict.clear()\n",
    "        for jth_cluster in sorted(self.load_balanced_view_dict):\n",
    "            self.cluster_RAM_use_dict[jth_cluster] = self.profile_memory_for_cluster(jth_cluster)           \n",
    "            self.logger_ram_usage.info('{}: {}th cluster uses {} GB of RAM'.format(\n",
    "                datetime.now(), jth_cluster, self.cluster_RAM_use_dict[jth_cluster]))\n",
    "        self.logger_ram_usage.info('{}: All clusters use {} GB of RAM'.format(\n",
    "                datetime.now(), sum(self.cluster_RAM_use_dict.values())))\n",
    "        \n",
    "    def clear_memory_on_cluster(self, cluster_id): # not as effective as imagined\n",
    "        import gc\n",
    "        self.cluster_dict[cluster_id][:].apply_async(gc.collect)\n",
    "        \n",
    "    def start_cluster(self, n_cpus, cluster_id):\n",
    "        self.logger_status.info(\"{}: \\tAttempting to start cluster job \"\n",
    "            \"{}'s {}th cluster with {} CPUs\".format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, n_cpus))\n",
    "        os.system(\"ipcluster start --n={} --profile={}{} --daemonize\".format(\n",
    "            n_cpus, self.cluster_job_name, cluster_id)) # should deprecate to use a safer OS call\n",
    "\n",
    "        attempt_ctr = 0 \n",
    "        while attempt_ctr < 3: # Attempt to connect to client 3 times\n",
    "            time.sleep(10) # hard coded\n",
    "            try:\n",
    "                cluster = ipp.Client(profile='{}{}'.format(self.cluster_job_name, cluster_id))\n",
    "            except ipp.error.TimeoutError:\n",
    "                attempt_ctr += 1\n",
    "            else:\n",
    "                self.cluster_pid_dict[cluster_id] = cluster[:].apply_async(os.getpid).get()\n",
    "                self.logger_status.info(('{}: \\t\\tCPU processes ready for action'\n",
    "                    ': {}').format(datetime.now(), self.cluster_pid_dict[cluster_id]))\n",
    "                return cluster\n",
    "            # if there is any other error other than TimeoutError, then the error will be raised\n",
    "            \n",
    "    def start_all_clusters(self):\n",
    "        self.activate_logger()\n",
    "        self.logger_status.info('{}: Starting Multiple Cluster Engine'.format(datetime.now()))\n",
    "        #self.logger_status.info('Attempting to start all clusters')\n",
    "        for cluster_id, n_cpus in enumerate(self.n_cpus_list):\n",
    "            self.cluster_dict[cluster_id] = self.start_cluster(n_cpus, cluster_id)\n",
    "            self.load_balanced_view_dict[cluster_id] = self.cluster_dict[cluster_id].load_balanced_view()            \n",
    "        self.start_time = datetime.now()\n",
    "        self.logger_status.info('{}: All clusters started at {}'.format(datetime.now(), self.start_time))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        \n",
    "    def kill_cluster(self, cluster_id): # use better arguments\n",
    "        self.logger_status.info((\"{}: \\tAttempting to kill cluster job {}'s {}th \"\n",
    "            \"cluster with CPU processes: {}\").format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, self.cluster_pid_dict[cluster_id]))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        # cluster.purge_everything() # sometimes takes forever\n",
    "        self.cluster_dict[cluster_id].close()\n",
    "        self.cluster_dict.pop(cluster_id)\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.logger_status.info('{}: \\t\\tCluster successfully killed'.format(datetime.now()))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        time.sleep(5) # hard-coded\n",
    "        \n",
    "    def kill_all_clusters(self):\n",
    "        self.end_time = datetime.now()\n",
    "        n_surviving_clusters = len(self.cluster_dict)\n",
    "        self.logger_status.info('{}: Killing all clusters'.format(datetime.now()))\n",
    "        for cluster_id in sorted(self.cluster_dict):\n",
    "            self.kill_cluster(cluster_id)\n",
    "        self.logger_status.info('{}: All clusters have been killed'.format(datetime.now()))\n",
    "        self.logger_status.info('{}: Multiple Cluster Engine shut down at {}'.format(\n",
    "            datetime.now(), self.end_time))\n",
    "        self.logger_status.info((\"{}: Processed {} files using {} surviving \"\n",
    "            \"clusters in {} minutes\").format(datetime.now(), len(self.input_file_names), \n",
    "            n_surviving_clusters, (self.end_time - self.start_time).seconds / 60.0))\n",
    "        logging.shutdown()\n",
    "        \n",
    "    def early_kill_cluster(self, cluster_id):\n",
    "        self.logger_failure.info((\"{}: Killing cluster job {}'s {}th cluster which \"\n",
    "            \"was processing file {} due to exceeding RAM limit\").format(\n",
    "            datetime.now(), self.cluster_job_name, cluster_id, \n",
    "            self.file_to_cluster_order_dict[cluster_id][-1]))\n",
    "        self.logger_ram_usage.info((\"{}: Killing cluster job {}'s {}th cluster due \"\n",
    "            \"to exceeding RAM limit\").format(datetime.now(), self.cluster_job_name, cluster_id))        \n",
    "        self.logger_status.info((\"{}: Killing cluster job {}'s {}th cluster with CPU \"\n",
    "            \"processes: {} due to exceeding RAM limit\").format(datetime.now(), \n",
    "            self.cluster_job_name, cluster_id, self.cluster_pid_dict[cluster_id]))\n",
    "        self.cluster_dict[cluster_id].close()\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        self.cluster_dict.pop(cluster_id)\n",
    "        self.async_results_dict.pop(cluster_id)\n",
    "        \n",
    "    def kill_cluster_if_ram_limit_exceeded(self): # only kills at max 1 cluster per method call\n",
    "        if sum(self.cluster_RAM_use_dict.values()) > self.ram_limit_in_GB:\n",
    "            cluster_id = sorted(self.cluster_RAM_use_dict, \n",
    "                                 key=self.cluster_RAM_use_dict.get, reverse=True)[0]\n",
    "            self.early_kill_cluster(cluster_id)\n",
    "            self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        assert len(self.load_balanced_view_dict) != 0, 'All clusters have been killed prematurely'\n",
    "        \n",
    "    def create_kwargs_dict_list(self, input_file_name, cluster_id, n_cpus):\n",
    "        function_kwargs_dict = copy.deepcopy(self.function_kwargs_dict)\n",
    "        function_kwargs_dict.update({'input_file_name': input_file_name,\n",
    "                                    'cluster_output_dir': self.cluster_output_dir,\n",
    "                                    'cluster_id': cluster_id,\n",
    "                                    'n_cpus': n_cpus})\n",
    "        function_kwargs_dict_list = []\n",
    "        for cpu_id in range(n_cpus):\n",
    "            function_kwargs_dict_list.append(copy.deepcopy(function_kwargs_dict))\n",
    "            function_kwargs_dict_list[cpu_id]['cpu_id'] = cpu_id\n",
    "        return function_kwargs_dict_list \n",
    "    \n",
    "    def check_if_function_in_cluster_failed(self, jth_cluster):\n",
    "        if self.async_results_dict[jth_cluster] == []: # cluster just started, so it\n",
    "            return # doesn't have any files sent to the cluster yet\n",
    "        else:\n",
    "            exception = self.async_results_dict[jth_cluster][-1].exception()\n",
    "            if exception:\n",
    "                self.logger_failure.info((\"{}: {}th cluster has error {} on \"\n",
    "                    \"file {}\").format(datetime.now(), jth_cluster, exception.args[0], \n",
    "                    self.file_to_cluster_order_dict[jth_cluster][-1]))\n",
    "                self.async_results_dict[jth_cluster].pop()\n",
    "                #self.async_results_dict.pop(jth_cluster)\n",
    "                                     \n",
    "    def run_clusters(self):\n",
    "        small_file_ctr = 1 # determine if you want to have queue or differently ordered queue\n",
    "        big_file_ctr = 0\n",
    "        \n",
    "        for ith_file in tqdm(range(len(self.input_file_names))):\n",
    "            while True:\n",
    "                time.sleep(self.wait_time_in_seconds)\n",
    "                self.profile_memory_for_all_clusters()\n",
    "                self.kill_cluster_if_ram_limit_exceeded()\n",
    "                jth_cluster = next(self.cluster_indexes)\n",
    "                \n",
    "                if (not self.async_results_dict[jth_cluster][-1:]\n",
    "                    or self.async_results_dict[jth_cluster][-1].done()): # check if cluster i is available                       \n",
    "                    self.clear_memory_on_cluster(jth_cluster)\n",
    "                    self.check_if_function_in_cluster_failed(jth_cluster) # check if previous file failed to process\n",
    "                    \n",
    "                    if jth_cluster == 0: # Send large files to large cluster (ALWAYS has id == 0)\n",
    "                        index = big_file_ctr\n",
    "                        big_file_ctr += 1\n",
    "                    else: # Send small files to small clusters (ALWAYS have id > 0)\n",
    "                        index = -small_file_ctr\n",
    "                        small_file_ctr += 1\n",
    "                                                                                   \n",
    "                    kwargs_dict_list = self.create_kwargs_dict_list(\n",
    "                        self.input_file_names[index],\n",
    "                        jth_cluster, \n",
    "                        len(self.cluster_dict[jth_cluster].ids))                    \n",
    "                    \n",
    "                    async_result = self.load_balanced_view_dict[jth_cluster].map_async(\n",
    "                        self.function_to_process, kwargs_dict_list)                                              \n",
    "                    self.async_results_dict[jth_cluster].append(async_result)\n",
    "                    self.file_to_cluster_order_dict[jth_cluster].append(self.input_file_names[index])\n",
    "                    # write status to file--it will only have start times, no end times\n",
    "                    self.logger_status.info((\"{}: {} is the {}th file and is sent to \"\n",
    "                        \"{}th cluster for processing\").format(datetime.now(),\n",
    "                        self.input_file_names[index], ith_file, jth_cluster))\n",
    "                    break # break out of inner loop to determine if other clusters are available\n",
    "\n",
    "        while not all(self.async_results_dict[jth_cluster][-1].done()\n",
    "                      for jth_cluster in self.async_results_dict): # wait for all clusters to finish\n",
    "            time.sleep(self.wait_time_in_seconds)\n",
    "            self.profile_memory_for_all_clusters()\n",
    "            self.kill_cluster_if_ram_limit_exceeded()\n",
    "                \n",
    "        cluster_set = set()\n",
    "        for jth_cluster in self.cluster_indexes:\n",
    "            if jth_cluster in cluster_set:\n",
    "                break\n",
    "            cluster_set.add(jth_cluster)\n",
    "            self.check_if_function_in_cluster_failed(jth_cluster) # check if last file failed to process\n",
    "        # async_results_dict; save to disk for later inspection? determine whether results takes too much RAM\n",
    "        \n",
    "    def main(self):\n",
    "        self.create_cluster_output_dir()\n",
    "        self.start_all_clusters()\n",
    "        self.run_clusters()\n",
    "        self.kill_all_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted from largest file to smallest, cluster for largest to smallest\n",
    "# if necessary, recreate engine here if cluster shut down\n",
    "# figure out queue vs deque; deque is better\n",
    "# # write a crap load of documentation\n",
    "# write shell script for configuration and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest with a mapper/reducer? after each map/reducer step, use gc.collect()\n",
    "\n",
    "# MCE works on files. Hence, if you don't have any datafiles, then just create some empty files\n",
    "# SSD for parallel reading (not HDD); determine if you are IO constrained\n",
    "# RAM usage is heavier in Python 3 than Python 2; though Python 3 memory management is better\n",
    "# during function failure: benefit (error type will be saved to failure.log) and weakness \n",
    "#     (it doesn't say what line code failed at so you have to debug your function outside \n",
    "#     of the MCE instance. You have to debug as if it were just calling the function by itself on some data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/donnemartin/data-science-ipython-notebooks/tree/master/mapreduce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
