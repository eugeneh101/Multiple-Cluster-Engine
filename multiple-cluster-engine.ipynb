{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decorator_1(func):\n",
    "    def func_wrapper(x):\n",
    "        print('dec1')\n",
    "        func(x)\n",
    "        print('dec1')\n",
    "    return func_wrapper\n",
    "\n",
    "def decorator_2(func):\n",
    "    def func_wrapper(x):\n",
    "        print('dec2')\n",
    "        func(x)\n",
    "        print('dec2')\n",
    "    return func_wrapper\n",
    "\n",
    "@decorator_2\n",
    "@decorator_1\n",
    "def function_a(x):\n",
    "    print(x)\n",
    "\n",
    "function_a(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_f(name1):\n",
    "    print('hello {}'.format(name1))\n",
    "\n",
    "def mapper_f(kwargs):\n",
    "    return temp_f(**kwargs)\n",
    "\n",
    "mapper_f({'name1': 'lee'})\n",
    "(lambda kwargs: temp_f(**kwargs))({'name1': 'lee'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !touch {i}.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "class MultipleClusterEnginePrototype(object):\n",
    "    def __init__(self, cluster_job_name, n_cpus_list, output_dir, file_names, function_to_process, function_kwargs_dict): # always put it in as a dictionary\n",
    "        self.cluster_job_name = cluster_job_name\n",
    "        self.n_cpus_list = n_cpus_list\n",
    "        self.output_dir = output_dir\n",
    "        self.file_names = file_names\n",
    "        self.function_to_process = lambda kwargs: function_to_process(**kwargs)\n",
    "        self.function_kwargs_dict = function_kwargs_dict\n",
    "        \n",
    "        assert cluster_job_name, \"Needs cluster name\"\n",
    "        assert len(n_cpus_list) > 0, \"Needs the number of CPUs per cluster\"\n",
    "        assert os.path.isdir(self.output_dir), \"Output directory doesn't exist\"\n",
    "        assert len(file_names) > 0, \"Need input files\"\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # used by engine\n",
    "        self.client_dict = {}\n",
    "        self.load_balanced_view_dict = {}\n",
    "        self.async_results_dict = defaultdict(list) # collects all the async_results\n",
    "        self.file_to_cluster_order_dict = defaultdict(list) # remembers which files are sent to which clusters\n",
    "        self.cluster_indexes = None\n",
    "        self.logger_status = None\n",
    "        self.logger_failure = None\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.cluster_output_dir = None\n",
    "        \n",
    "        \n",
    "            \n",
    "    def create_cluster_output_dir(self):\n",
    "        cluster_output_dir = os.path.join(self.output_dir, self.cluster_job_name)\n",
    "        if not os.path.isdir(cluster_output_dir):\n",
    "            os.makedirs(cluster_output_dir)        \n",
    "        subdirs = [name for name in os.listdir(cluster_output_dir) if \n",
    "                   os.path.isdir(os.path.join(cluster_output_dir, name))]\n",
    "        existing_results_dir = []\n",
    "        for subdir in subdirs:\n",
    "            try:\n",
    "                existing_results_dir.append(int(subdir))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        dir_index = max(existing_results_dir) + 1 if existing_results_dir else 0\n",
    "        self.cluster_output_dir = os.path.join(cluster_output_dir, str(dir_index))\n",
    "        os.makedirs(self.cluster_output_dir)\n",
    "                \n",
    "    def create_logger(self, logger_name, log_file, level=logging.INFO):\n",
    "        l = logging.getLogger(logger_name)\n",
    "        fileHandler = logging.FileHandler(log_file)\n",
    "        l.addHandler(fileHandler)\n",
    "        l.setLevel(level)\n",
    "    \n",
    "    def activate_logger(self):\n",
    "        #logging.basicConfig(filename='logger.log', level=logging.INFO)\n",
    "        #self.logger = logging.getLogger('Willie says') # Easter egg\n",
    "        self.create_logger('status', os.path.join(self.cluster_output_dir, \"status.log\"))\n",
    "        self.create_logger('failure', os.path.join(self.cluster_output_dir, \"failure.log\"))\n",
    "        self.logger_status = logging.getLogger('status')\n",
    "        self.logger_status.propagate = False\n",
    "        self.logger_failure = logging.getLogger('failure')\n",
    "        self.logger_failure.propagate = False\n",
    "        \n",
    "    def start_cluster(self, n_cpus, cluster_id):\n",
    "        self.logger_status.info('\\tAttempting to start {}{} with {} CPUs'.format(self.cluster_job_name, cluster_id, n_cpus))\n",
    "        os.system(\"ipcluster start --n={} --profile={}{} --daemonize\".format(\n",
    "            n_cpus, self.cluster_job_name, cluster_id)) # should deprecate to use a safer bash call\n",
    "\n",
    "        attempt_ctr = 0 \n",
    "        while attempt_ctr < 3: # Attempt to connect to client 3 times\n",
    "            time.sleep(10) # hard coded\n",
    "            try:\n",
    "                client = ipp.Client(profile='{}{}'.format(self.cluster_job_name, cluster_id))\n",
    "            except ipp.error.TimeoutError:\n",
    "                attempt_ctr += 1\n",
    "            else:\n",
    "                self.logger_status.info('\\t\\tCPU processes ready for action: {}'.format(client[:].apply_async(os.getpid).get()))\n",
    "                return client\n",
    "            # if there is any other error other than TimeoutError, then the error will be raised\n",
    "            \n",
    "    def start_all_clusters(self):\n",
    "        self.activate_logger()\n",
    "        self.logger_status.info('Starting Multiple Cluster Engine')\n",
    "        #self.logger_status.info('Attempting to start all clusters')\n",
    "        for cluster_id, n_cpus in enumerate(self.n_cpus_list):\n",
    "            self.client_dict[cluster_id] = self.start_cluster(n_cpus, cluster_id)\n",
    "            self.load_balanced_view_dict[cluster_id] = self.client_dict[cluster_id].load_balanced_view()            \n",
    "        self.start_time = datetime.now()\n",
    "        self.logger_status.info('All clusters started at {}'.format(self.start_time))\n",
    "        self.cluster_indexes = itertools.cycle(sorted(self.load_balanced_view_dict))\n",
    "        \n",
    "    def kill_cluster(self, cluster_id): # use better arguments\n",
    "        # client = client_list[cluster_id]\n",
    "        self.logger_status.info('\\tAttempting to kill {}{} with CPU processes: {}'.format(\n",
    "            self.cluster_job_name, cluster_id, self.client_dict[cluster_id][:].apply_async(os.getpid).get()))\n",
    "        self.load_balanced_view_dict.pop(cluster_id)\n",
    "        # client.purge_everything()\n",
    "        self.client_dict[cluster_id].close()\n",
    "        os.system('ipcluster stop --profile={}{}'.format(self.cluster_job_name, cluster_id))\n",
    "        self.logger_status.info('\\t\\tCluster successfully killed')\n",
    "        time.sleep(5) # hard-coded\n",
    "        # have to mutate cluster_indexes\n",
    "        \n",
    "    def kill_all_clusters(self):\n",
    "        self.end_time = datetime.now()\n",
    "        self.logger_status.info('Killing all clusters')\n",
    "        for cluster_id in self.client_dict:\n",
    "            self.kill_cluster(cluster_id)\n",
    "        self.logger_status.info('All clusters have been killed')\n",
    "        self.logger_status.info('Multiple Cluster Engine shut down at {}'.format(self.end_time))\n",
    "        self.logger_status.info('Total run time is {} minutes'.format((self.end_time - self.start_time).seconds / 60.0))\n",
    "        # logging.shutdown()\n",
    "        \n",
    "    def run_clusters(self):        \n",
    "        small_file_ctr = 1 # determine if you want to have queue or differently ordered queue\n",
    "        big_file_ctr = 0\n",
    "        \n",
    "\n",
    "        for ith_file in tqdm(range(len(self.file_names))):\n",
    "            for jth_cluster in self.cluster_indexes: # infinite loop\n",
    "                time.sleep(1) # hard coded delay time; want to do expected log time lag / number of clusters\n",
    "                ### insert code here to kill cluster if RAM usage too great, if possible log which file it was processing;\n",
    "                ### it has to do a global search of all clusters' RAM usage\n",
    "                ### would need a dictionary here to remember which cluster has which file; write to disk\n",
    "                ### profiler would also write to disk CPU usage what level\n",
    "\n",
    "                if (not self.async_results_dict[jth_cluster][-1:] \n",
    "                    or self.async_results_dict[jth_cluster][-1].done()): # check if cluster i is available                       \n",
    "                    # if necessary, recreate engine here\n",
    "                    if jth_cluster == 0: # Send large files to large cluster (ALWAYS has id == 0)\n",
    "                        index = big_file_ctr\n",
    "                        big_file_ctr += 1\n",
    "                    else: # Send small files to small clusters (ALWAYS have id > 0)\n",
    "                        index = -small_file_ctr\n",
    "                        small_file_ctr += 1\n",
    "                                              \n",
    "                                     \n",
    "                    # clear cluster memory \n",
    "\n",
    "                    # package_arguments\n",
    "                    function_kwargs_dict = copy.deepcopy(self.function_kwargs_dict)\n",
    "                    function_kwargs_dict.update({'file_name': self.file_names[index]}) \n",
    "                    \n",
    "                    ### insert code to write results to file--it will only have start times, no end times\n",
    "                    async_result = self.load_balanced_view_dict[jth_cluster].map_async(\n",
    "                            self.function_to_process, # function name\n",
    "#                            [self.file_names[index]] * len(self.client_dict[jth_cluster].ids), # file name, assumes first argument is always file name\n",
    "                            # [len(client_list[i].ids)] * len(client_list[i].ids), # number of CPUs, assumes second argument is always number of CPUs\n",
    "                            #  client_list[i].ids # CPU ids, assumes third argument is always CPU id; actually turn into kwargs\n",
    "                             # [output_folder_name] * len(client_list[i].ids) # assumes fourth argument is output directory\n",
    "#                            [self.function_kwargs_dict] * len(self.client_dict[jth_cluster].ids)                    \n",
    "                            [function_kwargs_dict] * len(self.client_dict[jth_cluster].ids)\n",
    "                            )                                              \n",
    "                    self.async_results_dict[jth_cluster].append(async_result)\n",
    "                    self.file_to_cluster_order_dict[jth_cluster].append(self.file_names[index])\n",
    "                    self.logger_status.info(\"{} is the {}th file and is sent to {}{} for processing\".format(self.file_names[index], ith_file, self.cluster_job_name, jth_cluster))\n",
    "                    break # break out of inner loop to determine if other clusters are available\n",
    "        # async_results_dict; save to disk for later inspection?\n",
    "        \n",
    "    def main(self):\n",
    "        # self.logger_failure.info('222ersaror foo')\n",
    "        self.create_cluster_output_dir()\n",
    "        self.start_all_clusters()\n",
    "        self.run_clusters()\n",
    "        self.kill_all_clusters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "def fun_func(file_name, save_string_to_file):\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(save_string_to_file)\n",
    "\n",
    "mce_args = {\n",
    "    'cluster_job_name': 'write_to_file', # no spaces\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'function_to_process': fun_func,\n",
    "    'function_kwargs_dict': {'save_string_to_file': 'pee-a-boo!'},\n",
    "    'output_dir': '/home/ubuntu/' # use absolute path since it's safer, has to exist\n",
    "    }\n",
    "\n",
    "mce = MultipleClusterEnginePrototype(**mce_args)\n",
    "# mce.start_all_clusters()\n",
    "# mce.run_clusters()\n",
    "mce.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pee-a-boo!pee-a-boo!pee-a-boo!"
     ]
    }
   ],
   "source": [
    "!cat 9.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mce.async_results_dict[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to kill all clusters\n",
      "\tAttempting to kill mycluster0 with CPU processes: [2065, 2066, 2068, 2072]\n",
      "\t\tCluster successfully killed\n",
      "\tAttempting to kill mycluster1 with CPU processes: [2136, 2137, 2139]\n",
      "\t\tCluster successfully killed\n",
      "\tAttempting to kill mycluster2 with CPU processes: [2196, 2198]\n",
      "\t\tCluster successfully killed\n",
      "All clusters have been killed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mce.kill_all_clusters()\n",
    "#!ipcluster stop --profile=mycluster2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018_02_26_01:39:22.406429'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(temp).replace('-','_').replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-02-26 01:39:22.406429'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-02-26 01:38:27.457637'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultipleClusterEngine(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.functions_to_run = functions_to_run\n",
    "        self.file_names = file_names\n",
    "        self.RAM_limit_in_GB = pass\n",
    "        # how to deal with other args/objects that functions might need\n",
    "        self.mce_job_name = None\n",
    "        pass\n",
    "   \n",
    "\n",
    "    def memory_profiler():\n",
    "        pass # if all clusters are dead, then raise Error with a message\n",
    "    \n",
    "    \n",
    "    def early_kill():\n",
    "        pass # write file to failure disk, maybe also cluster i and num_cpus\n",
    "    \n",
    "    def cluster_release_memory():\n",
    "        # after each map/reducer step, use gc.collect()\n",
    "        pass\n",
    "    \n",
    "    to_write = 'Processed ' + str(num_files) + ' files in ' \\\n",
    "            + str((datetime.today() - now).total_seconds() / 60) + ' minutes\\n'\n",
    "    to_write += 'Used ' + str(cpu_list[0]) + ' cpus to process ' + str(big_file_ctr) + \\\n",
    "                    ' big files and ' \n",
    "    # If there are multiple clusters, specify number of small files processed by small cluster\n",
    "    if len(cpu_list) > 1: \n",
    "        to_write += str(cpu_list[1]) + ' cpus to process ' + \\\n",
    "                    str(small_file_ctr) + ' small files.\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cluster is killed, then cannot trust jth_cluster index--instead of load_balanced_list, use load_balanced_dict\n",
    "\n",
    "# RAM writer and also progress/log writer and failre writer; use logging library\n",
    "# figure out queue vs deque; deque is better\n",
    "# # write a crap load of documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably no async or threading required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weakref\n",
    "# unittest with a mapper/reducer? after each map/reducer step, use gc.collect()\n",
    "\n",
    "# MCE works on files. Hence, if you don't have any datafiles, then just create some empty files\n",
    "# SSD for parallel reading (not HDD); determine if you are IO constrained\n",
    "# RAM usage is heavier in Python 3 than Python 2; though Python 3 memory management is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all engines killed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if function fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/donnemartin/data-science-ipython-notebooks/tree/master/mapreduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail -n 50 -f temp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/ubuntu/multiple-cluster-engine-in-python3',\n",
       "  ['.ipynb_checkpoints', '.git'],\n",
       "  ['README.md',\n",
       "   '4.tmp',\n",
       "   'status.log',\n",
       "   'aggregator_func.py',\n",
       "   '7.tmp',\n",
       "   'main.py',\n",
       "   '8.tmp',\n",
       "   'failure.log',\n",
       "   '9.tmp',\n",
       "   '5.tmp',\n",
       "   '6.tmp',\n",
       "   'multiple-cluster-engine.ipynb',\n",
       "   '.gitignore',\n",
       "   '1.tmp',\n",
       "   '0.tmp',\n",
       "   '2.tmp',\n",
       "   'RAM_Usage.ipynb',\n",
       "   '3.tmp']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.ipynb_checkpoints',\n",
       "  [],\n",
       "  ['multiple-cluster-engine-checkpoint.ipynb']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git',\n",
       "  ['hooks', 'logs', 'branches', 'info', 'objects', 'refs'],\n",
       "  ['packed-refs', 'config', 'COMMIT_EDITMSG', 'description', 'index', 'HEAD']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/hooks',\n",
       "  [],\n",
       "  ['post-update.sample',\n",
       "   'pre-applypatch.sample',\n",
       "   'pre-push.sample',\n",
       "   'pre-commit.sample',\n",
       "   'applypatch-msg.sample',\n",
       "   'pre-rebase.sample',\n",
       "   'update.sample',\n",
       "   'prepare-commit-msg.sample',\n",
       "   'commit-msg.sample']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/logs',\n",
       "  ['refs'],\n",
       "  ['HEAD']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/logs/refs',\n",
       "  ['heads', 'remotes'],\n",
       "  []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/logs/refs/heads',\n",
       "  [],\n",
       "  ['master']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/logs/refs/remotes',\n",
       "  ['origin'],\n",
       "  []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/logs/refs/remotes/origin',\n",
       "  [],\n",
       "  ['HEAD', 'master']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/branches', [], []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/info',\n",
       "  [],\n",
       "  ['exclude']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects',\n",
       "  ['d3',\n",
       "   '6b',\n",
       "   '7b',\n",
       "   '3e',\n",
       "   'cc',\n",
       "   '8e',\n",
       "   'c0',\n",
       "   '20',\n",
       "   'ee',\n",
       "   'b0',\n",
       "   'info',\n",
       "   '3a',\n",
       "   '98',\n",
       "   'e9',\n",
       "   '22',\n",
       "   '7c',\n",
       "   'pack',\n",
       "   'a4',\n",
       "   'ca',\n",
       "   '43',\n",
       "   '76',\n",
       "   '36',\n",
       "   '30',\n",
       "   '94',\n",
       "   'a7',\n",
       "   '09',\n",
       "   '1c',\n",
       "   'fc',\n",
       "   '61',\n",
       "   '4e',\n",
       "   'f5'],\n",
       "  []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/d3',\n",
       "  [],\n",
       "  ['fcdd32985910c1bc3bd0281caf36291f2e6ff8']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/6b',\n",
       "  [],\n",
       "  ['005bd84614bc6af72e50460ce2074330103189']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/7b',\n",
       "  [],\n",
       "  ['48a2849092b29b30eb1b207fac059b397b23a7']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/3e',\n",
       "  [],\n",
       "  ['c7cad6ef870195ff1ebb2ac166b50cc83a1c50']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/cc',\n",
       "  [],\n",
       "  ['18f27dc5f3e0f244defd140c917701033525e0']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/8e',\n",
       "  [],\n",
       "  ['cd5b908e9438cb9e59f97b665089479fc37647']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/c0',\n",
       "  [],\n",
       "  ['9e5c47ae0dd8432b986a462cba60e6edd43c62']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/20',\n",
       "  [],\n",
       "  ['94ab8335299bd586eca9cea94b4beccb7edc8f']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/ee',\n",
       "  [],\n",
       "  ['3e46b79c490f33d47e6074e6f17a39395287f4']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/b0',\n",
       "  [],\n",
       "  ['55141dac2b1d992dec59723089e6d035fb7f59']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/info', [], []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/3a',\n",
       "  [],\n",
       "  ['f577afe3947ed04386cc05af126dad4596f439']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/98',\n",
       "  [],\n",
       "  ['b1b1df67896bf577149d0236d2b786165f467a']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/e9',\n",
       "  [],\n",
       "  ['e8cfa537c967fa5dc3b14e531ae6d5bb01b943']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/22',\n",
       "  [],\n",
       "  ['dc7dff51401f12aaca4196239a0839c531cfec']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/7c',\n",
       "  [],\n",
       "  ['1422aaa63b163b0fd5189b2921f0ca88e79744']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/pack', [], []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/a4',\n",
       "  [],\n",
       "  ['074481d5391508e1adccefb332b11721273150']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/ca',\n",
       "  [],\n",
       "  ['e4132e27544244384394ec9a733d3fc084ce9c']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/43',\n",
       "  [],\n",
       "  ['3a965e03e1fa4623456e18b2ce6c24010c306f']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/76',\n",
       "  [],\n",
       "  ['3513e910f7036a1a3cdb21dce8e57da2451891']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/36',\n",
       "  [],\n",
       "  ['b815b90d5e4982dd66ab7feadc2f339ba9b621']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/30',\n",
       "  [],\n",
       "  ['a250b71a1eb7e11a459998b82d9e032600b2cc']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/94',\n",
       "  [],\n",
       "  ['e82005fcaef7957ee60e62ae242d3ec327fe3c']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/a7',\n",
       "  [],\n",
       "  ['97568da18dd2658122b67a5f88b50204edb705']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/09',\n",
       "  [],\n",
       "  ['298d46ff7297537806e1381278dda9d0425e42']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/1c',\n",
       "  [],\n",
       "  ['cfa1ab141ef668d71770eb4ab5d1c47ef97c74']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/fc',\n",
       "  [],\n",
       "  ['c9339aca773b48969e81dd28ad3920a49aa8cf']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/61',\n",
       "  [],\n",
       "  ['9d0028ed32fe6876fa09d6b3c0e7c4e8329c3b']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/4e',\n",
       "  [],\n",
       "  ['94c23c393649eee20d380892b562c6b6d6d254']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/objects/f5',\n",
       "  [],\n",
       "  ['4dc0e0a811bbde1a9fafbb809a34a5b66090f4']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/refs',\n",
       "  ['tags', 'heads', 'remotes'],\n",
       "  []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/refs/tags', [], []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/refs/heads',\n",
       "  [],\n",
       "  ['master']),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/refs/remotes',\n",
       "  ['origin'],\n",
       "  []),\n",
       " ('/home/ubuntu/multiple-cluster-engine-in-python3/.git/refs/remotes/origin',\n",
       "  [],\n",
       "  ['HEAD', 'master'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(os.walk(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/multiple-cluster-engine-in-python3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ssh',\n",
       " '.ipynb_checkpoints',\n",
       " '.bash_history',\n",
       " '.bashrc',\n",
       " '.bash_logout',\n",
       " 'Untitled1.ipynb',\n",
       " '.conda',\n",
       " 'anaconda2',\n",
       " 'Untitled2.ipynb',\n",
       " '.cache',\n",
       " '.viminfo',\n",
       " '.jupyter',\n",
       " 'cluster_results',\n",
       " '.ipython',\n",
       " '.w3m',\n",
       " '.bashrc-anaconda2.bak',\n",
       " '.continuum',\n",
       " '.config',\n",
       " 'multiple-cluster-engine-in-python3',\n",
       " '.local',\n",
       " '.python_history',\n",
       " '.Xauthority',\n",
       " '.profile']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/home/ubuntu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir('anaconda2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_dir = \"/home/ubuntu/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
