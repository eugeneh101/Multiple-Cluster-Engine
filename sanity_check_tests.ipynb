{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unittest with a mapper/reducer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multiple Tests Below\n",
    "Here are a couple tests to show how the engine works.  \n",
    "All code tested on AWS m4.2xlarge (8 CPUs and 32 GB of RAM) with Python 3. Also ran on Python 2 successfully; it appears that this code is Python 3 and 2 compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first make the output directory\n",
    "!mkdir /home/ubuntu/cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "__User-Defined Function (UDF)__: The user creates a function has the following mandatory arguments: ```input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id```. The MCE passes those keyword arguments into the UDF, so your UDF must take them in but doesn't have to use it. In this example UDF, ```save_a_string``` just saves to file a couple strings. Notice that the UDF must import any of the libraries it uses.  \n",
    "\n",
    "__MCE arguments__:  This must be a dictionary which has keyword arguments you pass into the MCE instance. It must has the following arguments (which everything you see in the example):  \n",
    "* cluster_job_name: what ever you like to name the cluster job. This will help create the final output directory, so use a string that doesn't have any weird characters or spaces. \n",
    "* n_cpus_list: number of CPUs for each cluster. Largest cluster in the beginning; for example: [5, 2, 2]\n",
    "* ram_limit_in_GB: threshold for total RAM usage across all clusters. If clusters use more RAM, then it will automatically kill the cluster that uses the most RAM.   \n",
    "* wait_time_in_seconds: the number of seconds between profiling RAM and determining whether if any cluster is available to send the next file. I just use 1 second, so it profiles every second. \n",
    "* input_file_names: a list of file names. Absolute paths are preferred as relative path not guaranteed to work. \n",
    "* output_parent_dir: Directory of the parent folder that will contain the final output directory. Absolute path preferred and this directory has to already exist.\n",
    "* function_to_process: Your UDF here\n",
    "* function_kwargs_dict: an argument dictionary that is specific to your UDF. If your UDF takes no extra arguments than the mandatory arguments, then put an empty dictionary here.  \n",
    "\n",
    "__MCE instance__: The instance of the MultipleClusterEngine class which you instantiate with your mce arguments dictionary. To run the whole thing, just call the main() method, which will call the following methods: \n",
    "* create_cluster_output_dir(): create the final output directory, which is based on your output_parent_dir + cluster_job_name + a uniquely incremented number\n",
    "* start_all_clusters(): starts all the clusters with the number of CPUs you set\n",
    "* run_clusters(): doles out the files to be processed by the clusters. Most of the runtime should be here\n",
    "* kill_all_clusters(): kills all the clusters when the processing is completed. \n",
    "\n",
    "## Debugging: \n",
    "If MCE.main() without error, the clusters should have been killed, everything is wrapped up, and everything is fine. Sometimes if your UDF fails or something weird happens, kill_all_clusters() won't work. You have to go ```htop``` or ```ps aux``` to see if the cluster is still running. If they are, you have to manually kill those processes with the following:\n",
    "```bash\n",
    "# kill each of the 3 clusters since n_cpus_list has length 3\n",
    "!ipcluster stop --profile save_to_string_0\n",
    "!ipcluster stop --profile save_to_string_1\n",
    "!ipcluster stop --profile save_to_string_2\n",
    "\n",
    "```\n",
    "Also, once a file has been sent to a cluster, you cannot stop it from inside Python. Hence, you have to manually kill the cluster to get the job to stop.\n",
    "\n",
    "## Results\n",
    "__```MCE.cluster_output_dir```__: This is the final output directory which is composed of ```output_parent_dir``` + ```cluster_job_name``` + a uniquely incremented number. Hence, it is safe to run the same code with same configurations, as the output will be written to a different final output directory. Moreover, you will know which is the latest run, as the directory will have the highest number.  \n",
    "__logging__: MCE will generate 3 logs which always include time stamps: ```status.log```, ```ram_usage.log```, ```failure.log```. \n",
    "* ```status.log``` will tell you when the clusters started, which files were sent to which clusters, if any clusters are prematurely killed, when the clusters are killed after processing has been completed, and how many files appear to be successfully processed on how many remaining clusters. If certain files failed to process successfully (for example, if a invoice file has a different delimiter than the others, then your function will fail), then the number of files successfully processed will be less than the length of ```input_file_names```. If a cluster was killed prematurely (probably due to exceeding RAM limit), then the number of surviving clusters will be less than length of ```n_cpus_list```. Basically, for a quick sanity check, see if number of files successfully processed equals the length of your input files--which would indicate everything worked out smoothly. Of course, please do separate sanity check and look at your files to make sure things actually did work out correctly. \n",
    "* ```ram_usage.log```: Every ```wait_time_in_seconds```, RAM usage for each cluster (and the sum to get all clusters) is recorded. If RAM usage exceeds ```ram_limit_in_GB```, then cluster will automatically kill the cluster using the most RAM and record here the cluster number and which file it was processing. \n",
    "* ```failure.log```: If a function failed on a file, then the cluster number, error type, and file name will be recorded here. Also, if RAM usage exceeds ```ram_limit_in_GB```, a message will be recorded here too. If everything worked out perfectly, this file will be empty. The goal is to have this file be empty.  \n",
    "\n",
    "__```MCE.async_results_dict```__: basically the instance attribute that records the history of everything MCE did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "        \n",
    "\n",
    "def save_a_string(string_saved_to_file, # actual args used in this function\n",
    "    # mandatory args, you can choose not to use them but function has to take them in\n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id  \n",
    "):\n",
    "    import os # function has to import all the libraries it uses\n",
    "    with open(input_file_name, 'r') as in_:\n",
    "        text = ''.join(in_.readlines())        \n",
    "    output_file_name = '_'.join(['cluster' + str(cluster_id), 'cpu' + str(cpu_id), input_file_name.split('/')[-1]])\n",
    "    output_file_name = os.path.join(cluster_output_dir, output_file_name)\n",
    "    with open(output_file_name, 'w') as out_:\n",
    "        out_.write('My cluster_id is {}\\n'.format(cluster_id))\n",
    "        out_.write('The number of CPUs in this cluster is {}\\n'.format(n_cpus))\n",
    "        out_.write('My CPU_id is {}\\n'.format(cpu_id))\n",
    "        out_.write('My string is: {}\\n'.format(string_saved_to_file))\n",
    "\n",
    "mce_args = {\n",
    "    'cluster_job_name': 'save_a_silly_string_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)], # absolute path preferred; I'm lazy\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # absolute path preferred, directory has to already exist\n",
    "    'function_to_process': save_a_string,\n",
    "    'function_kwargs_dict': {'string_saved_to_file': 'pee-a-boo!'} # function arguments here\n",
    "    }\n",
    "\n",
    "mce0 = MultipleClusterEngine(**mce_args)\n",
    "mce0.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             1: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             2: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce0.async_results_dict # contains the details of each AsyncResultObject such as time to complete, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Output Specifically for this Function\n",
    "__```mce0.cluster_output_dir```__: Since ```output_parent_dir``` is set to ```/home/ubuntu/cluster_results``` (which must already exist) and ```cluster_job_name``` is set to ```save_a_silly_string_```, the final output directory is ```/home/ubuntu/cluster_results/save_a_silly_string_``` + a uniquely incremented number. The first time you run this code, final output directory will be ```/home/ubuntu/cluster_results/save_a_silly_string_0```. The second time you run this code, final output directory will be ```/home/ubuntu/cluster_results/save_a_silly_string_1```.  \n",
    "__logging__:  if you want to see a log file update in real time, you can type ```tail -f status.log```\n",
    "* ```status.log```: has start and kill time of clusters. Shows which file went to which cluster. We inputted 10 files and asked for 3 clusters. Last line says \"Appears that 10 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". Looks like we are good.\n",
    "* ```ram_usage.log```: Every 1 second (```wait_time_in_seconds```), RAM usage for each cluster (and the sum to get entire MCE RAM usage) is recorded. Our RAM usage never got too high. If you wish, you can write a script to extract the timestamps and RAM usage to make a cool plot!    \n",
    "* ```failure.log```: Empty. That's what we want!  \n",
    "\n",
    "__```mce0.async_results_dict```__: basically the history of everything MCE did. For example:\n",
    "```python\n",
    "history = mce0.async_results_dict[0][-1] # get the history of cluster 0 and the last file sent to it.\n",
    "history.done() # returns True\n",
    "history.elapsed # returns 0.81518 seconds\n",
    "history.successful() # True since function worked. It will return False if there was an error\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "\n",
    "def error_func1( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in\n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id ):\n",
    "    1 / 0\n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'error_one_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': error_func1,\n",
    "    'function_kwargs_dict': {} # error_func1 takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce1 = MultipleClusterEngine(**mce_args)\n",
    "mce1.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [], 1: [], 2: []})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce1.async_results_dict # when a function fails on a file, its async_result history is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Output Specifically for this Function\n",
    "__```mce1.cluster_output_dir```__: Since ```output_parent_dir``` is set to ```/home/ubuntu/cluster_results``` (which must already exist) and ```cluster_job_name``` is set to ```error_one_```, the final output directory is ```/home/ubuntu/cluster_results/error_one_0``` the first time you run this code.  \n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 0 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". It's true since 0 files were processed because our UDF doesn't work. Fortunately, all 3 clusters survived. \n",
    "* ```ram_usage.log```: Every 1 second (```wait_time_in_seconds```), RAM usage for each cluster (and the sum to get all clusters) is recorded. Our RAM usage never got too high.\n",
    "* ```failure.log```: shows which file was sent to which cluster. Explains that the function hit a ```ZeroDivisionError```, which is just what we expect this UDF (```error_func1``` in this case) to have.  \n",
    "\n",
    "__```mce1.async_results_dict```__ is basically the history of everything MCE did. When a function fails on a file, the engine deletes the history for that file--I had to do it this way for the engine to work. That's why ```mce1.async_results_dict``` is equal to ```defaultdict(list, {0: [], 1: [], 2: []})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "\n",
    "def error_func2( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in \n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id):\n",
    "    '1' + 2\n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'error_two_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': error_func2,\n",
    "    'function_kwargs_dict': {} # error_func2 takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce2 = MultipleClusterEngine(**mce_args)\n",
    "mce2.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [], 1: [], 2: []})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce2.async_results_dict # when a function fails on a file, its async_result history is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Output Specifically for this Function\n",
    "__```mce2.cluster_output_dir```__: the final output directory is ```/home/ubuntu/cluster_results/error_two_0``` the first time you run this code.  \n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 0 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". It's true since 0 files were processed because our UDF doesn't work. Fortunately, all 3 clusters survived. \n",
    "* ```ram_usage.log```: Every 1 second, RAM usage for each cluster (and the sum to get all clusters) is recorded. Our RAM usage never got too high.\n",
    "* ```failure.log```: shows which file was sent to which cluster. Explains that the function hit a ```TypeError```, which is just what we expect this UDF (```error_func2``` adds a string and an integer) to have.  \n",
    "\n",
    "__```mce2.async_results_dict```__ is basically the history of everything MCE did. When a function fails on a file, the engine deletes the history for that file. That's why ```mce1.async_results_dict``` is equal to ```defaultdict(list, {0: [], 1: [], 2: []})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "    \n",
    "def exceed_memory_limit( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in \n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id):\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    if cluster_id % 2: # if cluster_id is odd, increase RAM usage until it is killed\n",
    "        exceed_ram_limit = []\n",
    "        current_value = 0\n",
    "        while True:\n",
    "            exceed_ram_limit.append(current_value)\n",
    "            current_value += 1\n",
    "    return cluster_id\n",
    "        \n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'exceed_memory_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2, 1, 1, 1], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': exceed_memory_limit,\n",
    "    'function_kwargs_dict': {} # exceed_memory_limit takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce3 = MultipleClusterEngine(**mce_args)\n",
    "mce3.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             2: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             4: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce3.async_results_dict # when a function fails due to exceeding memory limit, its async_result history is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explanation of Output Specifically for this Function\n",
    "__```mce3.cluster_output_dir```__: the final output directory is ```/home/ubuntu/cluster_results/exceed_memory_0``` the first time you run this code.  \n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 0 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". It's true since 0 files were processed because our UDF doesn't work. Fortunately, all 3 clusters survived. \n",
    "* ```ram_usage.log```: Every 1 second, RAM usage for each cluster (and the sum to get all clusters) is recorded. Our RAM usage never got too high.\n",
    "* ```failure.log```: shows which file was sent to which cluster. Explains that the function hit a ```TypeError```, which is just what we expect this UDF (```error_func2``` adds a string and an integer) to have.  \n",
    "\n",
    "__```mce2.async_results_dict```__ is basically the history of everything MCE did. When a function fails on a file, the engine deletes the history for that file. That's why ```mce1.async_results_dict``` is equal to ```defaultdict(list, {0: [], 1: [], 2: []})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:03<00:02,  1.97it/s]"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "All clusters have been killed prematurely",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc6f7d903208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmce4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultipleClusterEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmce_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmce4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# I'm expecting an Exception here due to killing all clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/multiple-cluster-engine-in-python3/multiple_cluster_engine.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_cluster_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_all_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_all_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multiple-cluster-engine-in-python3/multiple_cluster_engine.py\u001b[0m in \u001b[0;36mrun_clusters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time_in_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_memory_for_all_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_cluster_if_ram_limit_exceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                 \u001b[0mjth_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multiple-cluster-engine-in-python3/multiple_cluster_engine.py\u001b[0m in \u001b[0;36mkill_cluster_if_ram_limit_exceeded\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m             self.logger_status.info((\"{}: All clusters have been killed prematurely \"\n\u001b[1;32m    194\u001b[0m                 \"(probably due to exceeding RAM limit)\").format(datetime.now()))\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All clusters have been killed prematurely'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_if_function_in_cluster_failed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjth_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: All clusters have been killed prematurely"
     ]
    }
   ],
   "source": [
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "\n",
    "def exceed_memory_limit_all( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in \n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id):\n",
    "    import time\n",
    "    time.sleep(cluster_id * cpu_id)\n",
    "    exceed_ram_limit = []\n",
    "    current_value = 0\n",
    "    while True:\n",
    "        exceed_ram_limit.append(current_value)\n",
    "        current_value += 1\n",
    "    return cluster_id\n",
    "        \n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'exceed_memory_all_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2, 1, 1, 1], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': exceed_memory_limit_all,\n",
    "    'function_kwargs_dict': {} # exceed_memory_limit takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce4 = MultipleClusterEngine(**mce_args)\n",
    "mce4.main() # I'm expecting an Exception here due to killing all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce4.async_results_dict # when a function fails due to exceeding memory limit, its async_result history is removed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
