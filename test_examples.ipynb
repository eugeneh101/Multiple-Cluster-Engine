{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multiple Tests Below\n",
    "Here are a couple tests to show how the engine works.  \n",
    "All code tested on AWS m4.2xlarge (8 CPUs and 32 GB of RAM) with Python 3. Also ran on Python 2 successfully; it appears that this code is Python 3 and 2 compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first make the output directory\n",
    "!mkdir /home/ubuntu/cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "__User-Defined Function (UDF)__: The user creates a function that is the MCE argument ```function_to_process``` which has the following mandatory arguments: ```input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id```. The MCE passes those keyword arguments into the UDF, so your UDF must take them in but doesn't have to use it. In the following example UDF, ```save_a_string``` just saves to file a couple strings. Notice that the UDF must import any of the libraries it uses.  \n",
    "\n",
    "__MCE arguments__:  This must be a dictionary which has keyword arguments you pass into the MCE instance. It must has the following arguments (which everything you see in the example):  \n",
    "* cluster_job_name: what ever you like to name the cluster job. This will help create the final output directory, so use a string that doesn't have any weird characters or spaces. \n",
    "* n_cpus_list: number of CPUs for each cluster. Largest cluster in the beginning; for example: [5, 2, 2]. You should rarely make a cluster with 1 CPU because then it would defeat the purpose of multi-CPU map-reduce.\n",
    "* ram_limit_in_GB: threshold for total RAM usage across all clusters. If clusters use more RAM, then it will automatically kill the cluster that uses the most RAM.   \n",
    "* wait_time_in_seconds: the number of seconds between profiling RAM and determining whether if any cluster is available to send the next file. I just use 1 second, so it profiles every second. \n",
    "* input_file_names: a list of file names. Absolute paths are preferred as relative path not guaranteed to work. \n",
    "* output_parent_dir: Directory of the parent folder that will contain the final output directory. Absolute path preferred and this directory has to already exist.\n",
    "* function_to_process: Your UDF here\n",
    "* function_kwargs_dict: an argument dictionary that is specific to your UDF. If your UDF takes no extra arguments than the mandatory arguments, then put an empty dictionary here.  \n",
    "\n",
    "__MCE instance__: The instance of the MultipleClusterEngine class which you instantiate with your mce arguments dictionary. To run the whole thing, just call the main() method, which will call the following methods: \n",
    "* create_cluster_output_dir(): create the final output directory, which is based on your output_parent_dir + cluster_job_name + a uniquely incremented number\n",
    "* start_all_clusters(): starts all the clusters with the number of CPUs you set\n",
    "* run_clusters(): doles out the files to be processed by the clusters. Most of the runtime should be here\n",
    "* kill_all_clusters(): kills all the clusters when the processing is completed. \n",
    "\n",
    "## Debugging: \n",
    "If MCE.main() finishes without error, the clusters will naturally be killed and everything is fine. Sometimes if your UDF fails or something weird happens, kill_all_clusters() won't work. You have to go ```htop``` or ```ps aux``` to see if the cluster is still running. For the following example, to manually kill the clusters:\n",
    "```bash\n",
    "# kill each of the 3 clusters since n_cpus_list has length 3\n",
    "!ipcluster stop --profile save_to_string_0\n",
    "!ipcluster stop --profile save_to_string_1\n",
    "!ipcluster stop --profile save_to_string_2\n",
    "\n",
    "```\n",
    "Also, once a file has been sent to a cluster, you cannot stop it from inside Python. Hence, you have to manually kill the cluster to get the job to stop.\n",
    "\n",
    "## Results\n",
    "__```MCE.cluster_output_dir```__: This is the final output directory which is composed of ```output_parent_dir``` + ```cluster_job_name``` + a uniquely incremented number. Hence, it is safe to run the same code with same configurations, as the output will be written to a different final output directory. Moreover, you will know which is the latest run, as the directory will have the highest number.  \n",
    "__logging__: MCE will generate 3 logs which always include time stamps: ```status.log```, ```ram_usage.log```, ```failure.log```. \n",
    "* ```status.log``` will tell you when the clusters started, which files were sent to which clusters, if any clusters are prematurely killed, when the clusters are killed after processing has been completed, and how many files appear to be successfully processed on how many remaining clusters. If certain files failed to process successfully (for example, if a invoice file has a different delimiter than the others, then your function will fail), then the number of files successfully processed will be less than the length of ```input_file_names```. If a cluster was killed prematurely (probably due to exceeding RAM limit), then the number of surviving clusters will be less than length of ```n_cpus_list```. Basically, for a quick sanity check, see if number of files successfully processed equals the length of your input files--which would indicate everything worked out smoothly. Of course, please do separate sanity check and look at your files to make sure things actually did work out correctly. \n",
    "* ```ram_usage.log```: Every ```wait_time_in_seconds```, RAM usage for each cluster (and the sum to get all clusters) is recorded. If RAM usage exceeds ```ram_limit_in_GB```, then cluster will automatically kill the cluster using the most RAM and record here the cluster number and which file it was processing. \n",
    "* ```failure.log```: If a function failed on a file, then the cluster number, error type, and file name will be recorded here. Also, if RAM usage exceeds ```ram_limit_in_GB```, a message will be recorded here too. If everything worked out perfectly, this file will be empty. The goal is to have this file be empty.  \n",
    "\n",
    "__```MCE.async_results_dict```__: basically the instance attribute that records the history of everything MCE did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 343 ms, sys: 75.4 ms, total: 418 ms\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "        \n",
    "\n",
    "def save_a_string(string_saved_to_file, # actual args used in this function\n",
    "    # mandatory args, you can choose not to use them but function has to take them in\n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id  \n",
    "):\n",
    "    import os # function has to import all the libraries it uses\n",
    "    with open(input_file_name, 'r') as in_:\n",
    "        text = ''.join(in_.readlines())        \n",
    "    output_file_name = '_'.join(['cluster' + str(cluster_id), 'cpu' + str(cpu_id), input_file_name.split('/')[-1]])\n",
    "    output_file_name = os.path.join(cluster_output_dir, output_file_name)\n",
    "    with open(output_file_name, 'w') as out_:\n",
    "        out_.write('My cluster_id is {}\\n'.format(cluster_id))\n",
    "        out_.write('The number of CPUs in this cluster is {}\\n'.format(n_cpus))\n",
    "        out_.write('My CPU_id is {}\\n'.format(cpu_id))\n",
    "        out_.write('My string is: {}\\n'.format(string_saved_to_file))\n",
    "\n",
    "mce_args = {\n",
    "    'cluster_job_name': 'save_a_silly_string_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)], # absolute path preferred; I'm lazy\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # absolute path preferred, directory has to already exist\n",
    "    'function_to_process': save_a_string,\n",
    "    'function_kwargs_dict': {'string_saved_to_file': 'pee-a-boo!'} # function arguments here\n",
    "    }\n",
    "\n",
    "mce0 = MultipleClusterEngine(**mce_args)\n",
    "mce0.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             1: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             2: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce0.async_results_dict # contains the details of each AsyncResultObject such as time to complete, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Results Specifically for this Function\n",
    "__```mce0.cluster_output_dir```__: Since ```output_parent_dir``` is set to ```/home/ubuntu/cluster_results``` (which must already exist) and ```cluster_job_name``` is set to ```save_a_silly_string_```, the final output directory is ```/home/ubuntu/cluster_results/save_a_silly_string_``` + a uniquely incremented number. The first time you run this code, final output directory will be ```/home/ubuntu/cluster_results/save_a_silly_string_0```. The second time you run this code, final output directory will be ```/home/ubuntu/cluster_results/save_a_silly_string_1```.  \n",
    "__logging__:  if you want to see a log file update in real time, you can type ```tail -f status.log```\n",
    "* ```status.log```: has start and kill time of clusters. Shows which file went to which cluster. We inputted 10 files and asked for 3 clusters. Last line says \"Appears that 10 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". Looks like we are good.\n",
    "* ```ram_usage.log```: Every 1 second (```wait_time_in_seconds```), RAM usage for each cluster (and the sum to get entire MCE RAM usage) is recorded. Our RAM usage never got too high. If you wish, you can write a script to extract the timestamps and RAM usage to make a cool plot!    \n",
    "* ```failure.log```: Empty. That's what we want!  \n",
    "\n",
    "__```mce0.async_results_dict```__: basically the history of everything MCE did. For example:\n",
    "```python\n",
    "history = mce0.async_results_dict[0][-1] # get the history of cluster 0 and the last file sent to it.\n",
    "history.done() # returns True\n",
    "history.elapsed # returns 0.81518 seconds\n",
    "history.successful() # True since function worked. It will return False if there was an error\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 336 ms, sys: 35.5 ms, total: 372 ms\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "\n",
    "def error_func1( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in\n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id ):\n",
    "    1 / 0\n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'error_one_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': error_func1,\n",
    "    'function_kwargs_dict': {} # error_func1() takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce1 = MultipleClusterEngine(**mce_args)\n",
    "mce1.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [], 1: [], 2: []})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce1.async_results_dict # when a function fails on a file, its async_result history is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Results Specifically for this Function\n",
    "__```mce1.cluster_output_dir```__: Since ```output_parent_dir``` is set to ```/home/ubuntu/cluster_results``` (which must already exist) and ```cluster_job_name``` is set to ```error_one_```, the final output directory is ```/home/ubuntu/cluster_results/error_one_0``` the first time you run this code.  \n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 0 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". It's true since 0 files were processed because our UDF doesn't work. Fortunately, all 3 clusters survived. \n",
    "* ```ram_usage.log```: Every 1 second (```wait_time_in_seconds```), RAM usage for each cluster (and the sum to get all clusters) is recorded. Our RAM usage never got too high.\n",
    "* ```failure.log```: shows which file was sent to which cluster when something bad occurred. Explains that the function hit a ```ZeroDivisionError```, which is just what we expect this UDF (```error_func1()``` in this case) to have.  \n",
    "\n",
    "__```mce1.async_results_dict```__: basically the history of everything MCE did. When a function fails on a file, the engine deletes the history for that file--I had to do it this way for the engine to work. That's why ```mce1.async_results_dict``` is equal to ```defaultdict(list, {0: [], 1: [], 2: []})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 312 ms, sys: 68.6 ms, total: 380 ms\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "\n",
    "def error_func2( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in \n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id):\n",
    "    '1' + 2\n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'error_two_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': error_func2,\n",
    "    'function_kwargs_dict': {} # error_func2() takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce2 = MultipleClusterEngine(**mce_args)\n",
    "mce2.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [], 1: [], 2: []})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce2.async_results_dict # when a function fails on a file, its async_result history is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Results Specifically for this Function\n",
    "__```mce2.cluster_output_dir```__: the final output directory is ```/home/ubuntu/cluster_results/error_two_0``` the first time you run this code.  \n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 0 files were successfully processed using 3 surviving clusters in 0.18333333333333332 minutes\". It's true since 0 files were processed because our UDF doesn't work. Fortunately, all 3 clusters survived. \n",
    "* ```ram_usage.log```: Every 1 second, RAM usage for each cluster (and the sum to get all clusters) is recorded. Our RAM usage never got too high.\n",
    "* ```failure.log```: shows which file was sent to which cluster when something bad occurred. Explains that the function hit a ```TypeError```, which is just what we expect this UDF (```error_func2()``` adds a string and an integer) to have.  \n",
    "\n",
    "__```mce2.async_results_dict```__: basically the history of everything MCE did. When a function fails on a file, the engine deletes the history for that file. That's why ```mce2.async_results_dict``` is equal to ```defaultdict(list, {0: [], 1: [], 2: []})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 385 ms, sys: 73.1 ms, total: 458 ms\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "    \n",
    "def exceed_memory_limit( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in \n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id):\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    if cluster_id % 2: # if cluster_id is odd, increase RAM usage until it is killed\n",
    "        exceed_ram_limit = []\n",
    "        current_value = 0\n",
    "        while True:\n",
    "            exceed_ram_limit.append(current_value)\n",
    "            current_value += 1\n",
    "    return cluster_id\n",
    "        \n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'exceed_memory_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2, 1, 1, 1], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': exceed_memory_limit,\n",
    "    'function_kwargs_dict': {} # exceed_memory_limit()takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce3 = MultipleClusterEngine(**mce_args)\n",
    "mce3.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             2: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             4: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce3.async_results_dict # when a function fails due to exceeding memory limit, its async_result history is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Results Specifically for this Function\n",
    "__```mce3.cluster_output_dir```__: the final output directory is ```/home/ubuntu/cluster_results/exceed_memory_0``` the first time you run this code.  \n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 7 files were successfully processed using 3 surviving clusters in 1.0166666666666666 minutes\". We gave 10 files and 7 were correctly processed. This is what we expected since ```exceed_memory_limit()``` is specifically designed to increase RAM indefinitely on odd numbered clusters. We started with 6 clusters and ended up with 3. Our RAM profiler and ```ram_limit_in_GB``` worked to automatically kill cluster 1, 3, and 5. For example, one line says:  \"Killing cluster job exceed\\_memory\\_'s 1th cluster with CPU processes: [4126, 4128, 4130] due to exceeding RAM limit\"\n",
    "* ```ram_usage.log```: Every 1 second, RAM usage for each cluster (and the sum to get all clusters) is recorded. Shows when RAM gets too high, a cluster is killed. For example, the last line is \"Killing cluster job exceed\\_memory\\_'s 5th cluster due to exceeding RAM limit\"\n",
    "* ```failure.log```: Explains that the function (```exceed_memory_limit()```) hit memory limits on which clusters processing which files. These files would have to be processed again.\n",
    "\n",
    "__```mce3.async_results_dict```__: basically the history of everything MCE did. In the dictionary values, there are only 7 elements in the lists representing the 7 files that were successfully processed. 3 files were sent to clusters that were killed prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## UDF Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:06<00:04,  1.01s/it]"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "All clusters have been killed prematurely",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/multiple-cluster-engine-in-python3/multiple_cluster_engine.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_cluster_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_all_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_all_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multiple-cluster-engine-in-python3/multiple_cluster_engine.py\u001b[0m in \u001b[0;36mrun_clusters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time_in_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_memory_for_all_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_cluster_if_ram_limit_exceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mjth_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 if (not self.async_results_dict[jth_cluster][-1:]\n",
      "\u001b[0;32m~/multiple-cluster-engine-in-python3/multiple_cluster_engine.py\u001b[0m in \u001b[0;36mkill_cluster_if_ram_limit_exceeded\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m             self.logger_status.info((\"{}: All clusters have been killed prematurely \"\n\u001b[1;32m    217\u001b[0m                 \"(probably due to exceeding RAM limit)\").format(datetime.now()))\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All clusters have been killed prematurely'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_if_function_in_cluster_failed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: All clusters have been killed prematurely"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "\n",
    "# create some fake files\n",
    "for i in range(10):\n",
    "    !echo {i + 10} > {i}.tmp        \n",
    "\n",
    "\n",
    "def exceed_memory_limit_all( # this function takes no real args that are not mandatory\n",
    "    # mandatory args, you can choose not to use them but function has to take them in \n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id):\n",
    "    import time\n",
    "    time.sleep(cluster_id * cpu_id)\n",
    "    exceed_ram_limit = []\n",
    "    current_value = 0\n",
    "    while True:\n",
    "        exceed_ram_limit.append(current_value)\n",
    "        current_value += 1\n",
    "    return cluster_id\n",
    "        \n",
    "    \n",
    "mce_args = {\n",
    "    'cluster_job_name': 'exceed_memory_all_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2, 1, 1, 1], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': ['{}.tmp'.format(i) for i in range(10)],\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': exceed_memory_limit_all,\n",
    "    'function_kwargs_dict': {} # exceed_memory_limit_all() takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce4 = MultipleClusterEngine(**mce_args)\n",
    "mce4.main() # I'm expecting an Exception here due to killing all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce4.async_results_dict # when a function fails due to exceeding memory limit, its async_result history is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Results Specifically for this Function\n",
    "__```mce4.cluster_output_dir```__: the final output directory is ```/home/ubuntu/cluster_results/exceed_memory_all_1``` the first time you run this code. Weird. It should be ```exceed_memory_all_0```. Must be a small bug. \n",
    "\n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \" All clusters have been killed prematurely (probably due to exceeding RAM limit)\". We started 6 clusters but all of them were killed due to our function, which I coded the engine to raise an Exception. This is what we expected. \n",
    "* ```ram_usage.log```: Every 1 second, RAM usage for each cluster (and the sum to get all clusters) is recorded. Shows when RAM gets too high, a cluster is killed. You see 1 by 1, all the clusters are eventually killed.  \n",
    "* ```failure.log```: Explains that the function (```exceed_memory_limit_all()```) hit memory limits on which clusters processing which files. These files would have to be processed again. All remaining files that weren't sent to a cluster would also have to be processed again. Do a strong sanity check here to see if any of the resulting output files make any sense/work processed correctly.\n",
    "\n",
    "__```mce4.async_results_dict```__: Since none of the files were processed, they were all removed from the dictionary values. Whenever a cluster hits RAM limit and is prematurely killed, the cluster is removed from the defaultdict (ie its cluster number will be popped from defaultdict). Hence, this defaultdict dict is completely empty without any keys. The drawback is suppose if cluster 4 processed 5 files correctly and then hits a memory limit on the 6th file, then all 6 files history in ```mce4.async_results_dict``` will be deleted (```mce4.async_results_dict.pop(4)``` will be called when cluster killed prematurely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Example 5: Actual Map-Reduce Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.1 s, sys: 140 ms, total: 27.3 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create some fake data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!mkdir fake_data\n",
    "\n",
    "for i in range(10):\n",
    "    userID = np.random.randint(1000, size=1000000)\n",
    "    invoice_payment = np.random.randint(100000, size=1000000)\n",
    "\n",
    "    df = pd.DataFrame({'userID': userID, 'invoice_payment': invoice_payment})\n",
    "    df.to_csv('fake_data/{}.tmp'.format(i), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_invoice_total(\n",
    "    # mandatory args, you can choose not to use them but function has to take them in\n",
    "     input_file_name, cluster_output_dir, cluster_id, n_cpus, cpu_id  \n",
    "):\n",
    "    import os # function has to import all the libraries it uses\n",
    "    from collections import defaultdict\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "    \n",
    "    def mapper(input_file_name, n_cpus, cpu_id):\n",
    "        data_dict = defaultdict(list)\n",
    "        with open(input_file_name, 'r') as in_:\n",
    "            for line in in_:\n",
    "                line = line.strip().split(',')\n",
    "                userID, invoice_payment = int(line[0]), int(line[1])\n",
    "                if hash(userID) % n_cpus == cpu_id:\n",
    "                    data_dict[userID].append(invoice_payment)\n",
    "        return data_dict\n",
    "    \n",
    "    def reducer(data_dict, cluster_output_dir, cluster_id, cpu_id):\n",
    "        results_dict = {}\n",
    "        for userID in sorted(data_dict):\n",
    "            data = data_dict.pop(userID) # reduces data_dict size\n",
    "            results_dict[userID] = [pd.DataFrame(data).sum()[0]] # only create a DataFrame for 1 userID at a time\n",
    "            \n",
    "        output_file_name = '_'.join(['cluster' + str(cluster_id), \n",
    "             'cpu' + str(cpu_id), input_file_name.split('/')[-1]])\n",
    "        output_file_name = os.path.join(cluster_output_dir, output_file_name)\n",
    "        pd.DataFrame(results_dict).T.to_csv(output_file_name, header=False)\n",
    "\n",
    "    \n",
    "    data_dict = mapper(input_file_name, n_cpus, cpu_id)\n",
    "    gc.collect() # garbage collector might reduce RAM usage; quick to run\n",
    "    reducer(data_dict, cluster_output_dir, cluster_id, cpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:28<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 536 ms, sys: 65 ms, total: 601 ms\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiple_cluster_engine import MultipleClusterEngine\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "input_file_names = glob('fake_data/*')\n",
    "mce_args = {\n",
    "    'cluster_job_name': 'get_user_invoice_total_', # no spaces as it will be part of directory name\n",
    "    'n_cpus_list': [4, 3, 2], # 1st cluster is always the largest or equal to the other clusters\n",
    "    'ram_limit_in_GB': 20.0,\n",
    "    'wait_time_in_seconds': 1,\n",
    "    'input_file_names': input_file_names,\n",
    "    'output_parent_dir': '/home/ubuntu/cluster_results', # use absolute path since it's safer, has to already exist\n",
    "    'function_to_process': user_invoice_total,\n",
    "    'function_kwargs_dict': {} # user_invoice_total() takes no additional arguments    \n",
    "    }\n",
    "\n",
    "mce5 = MultipleClusterEngine(**mce_args)\n",
    "mce5.main() # I'm expecting an Exception here due to killing all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             1: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>],\n",
       "             2: [<AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>,\n",
       "              <AsyncMapResult: <lambda>:finished>]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mce5.async_results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I Debug my UDF when there is an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 762 ms, total: 25.7 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Basically, I put print statements in my UDF until the function seems to work.\n",
    "# You can also have the UDF print how long the mapping and reducing steps took to see if \n",
    "# you are IO constrained by the mapper, or CPU constrained by the reducer.\n",
    "\n",
    "!mkdir test_output\n",
    "user_invoice_total(**{\n",
    "    'input_file_name': 'fake_data/0.tmp',\n",
    "    'cluster_output_dir': '/home/ubuntu/multiple-cluster-engine-in-python3/test_output',\n",
    "    'cluster_id': 2,\n",
    "    'n_cpus': 2,\n",
    "    'cpu_id': 0,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the resulting output files are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCE results match manual extraction for file: 0.tmp: True\n",
      "MCE results match manual extraction for file: 1.tmp: True\n",
      "MCE results match manual extraction for file: 2.tmp: True\n",
      "MCE results match manual extraction for file: 3.tmp: True\n",
      "MCE results match manual extraction for file: 4.tmp: True\n",
      "MCE results match manual extraction for file: 5.tmp: True\n",
      "MCE results match manual extraction for file: 6.tmp: True\n",
      "MCE results match manual extraction for file: 7.tmp: True\n",
      "MCE results match manual extraction for file: 8.tmp: True\n",
      "MCE results match manual extraction for file: 9.tmp: True\n",
      "CPU times: user 3.89 s, sys: 92.6 ms, total: 3.98 s\n",
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    original_data = pd.read_csv('fake_data/{}.tmp'.format(i), header=None)\n",
    "\n",
    "    original_data_manually_processed = pd.read_csv('fake_data/{}.tmp'.format(i), header=None)\n",
    "    original_data_manually_processed = original_data.groupby(0)[1].sum()\n",
    "    original_data_manually_processed = original_data_manually_processed.sort_index().reset_index()\n",
    "\n",
    "    mce_processed_data = []\n",
    "    for file_ in glob('/home/ubuntu/cluster_results/get_user_invoice_total_1/*{}.tmp'.format(i)):\n",
    "        mce_processed_data.append(pd.read_csv(file_, header=None))\n",
    "\n",
    "    mce_processed_data = pd.concat(mce_processed_data)\n",
    "    mce_processed_data.sort_values(0, inplace=True)\n",
    "    mce_processed_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print('MCE results match manual extraction for file: {}.tmp: {}'.format(i,\n",
    "        pd.DataFrame.equals(original_data_manually_processed, mce_processed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Results Specifically for this Function\n",
    "__```mce5.cluster_output_dir```__: the final output directory is ```/home/ubuntu/cluster_results/get_user_invoice_total_1``` the first time you run this code. Weird. It should be ```get_user_invoice_total_1```. Must be a small bug. \n",
    "\n",
    "__logging__:  \n",
    "* ```status.log```: Last line: \"Appears that 10 files were successfully processed using 3 surviving clusters in 1.8 minutes\". We sent in 10 files and 10 files were successfully processed. We created 3 clusters and 3 survived to the end. Great!\n",
    "* ```ram_usage.log```: Every 1 second, RAM usage for each cluster (and the sum to get all clusters) is recorded. RAM never gets very high. \n",
    "* ```failure.log```: Empty. Perfect! \n",
    "\n",
    "__```mce5.async_results_dict```__: basically the history of everything MCE did. For example:\n",
    "```python\n",
    "history0 = mce5.async_results_dict[0][-1] # get the history of cluster 0 and the last file sent to it.\n",
    "history1 = mce5.async_results_dict[2][-1] # get the history of cluster 2 and the last file sent to it.\n",
    "print(history0.elapsed, history1.elapsed) # prints 17.952412 30.098238. This makes sense because cluster 0 had 4 CPUs and cluster 2 had 2 clusters. More clusters means faster processing times.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
